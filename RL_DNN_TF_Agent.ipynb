{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import pandas_ta\n","import numpy as np\n"," \n","# from gym_anytrading.datasets import FOREX_EURUSD_1H_ASK, STOCKS_GOOGL\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import time\n","import datetime\n","from six.moves import range\n","\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["PRICE_COLUMN = 'close'\n","USE_PRICE_RANGE_COLUMNS = False\n","\n","freq = 'h'\n","start_date='2016-01-02'\n","end_date='2019-03-28'\n","\n","ranges_dict_path = 'data\\\\ranges_dict.pickle'\n","save_path = f'.\\\\data\\\\featured_prices_{freq}_start_{start_date}.csv'\n","\n","# prices_path = '.\\\\data\\\\prices_freq-min_2019-01-01_2019-03-28.csv'\n","prices_path = '.\\\\data\\\\sources\\\\coinbaseUSD_1-min_data_2014-12-01_to_2019-01-09.csv'\n","\n","# Scrapped from twitters from 2016-01-01 to 2019-03-29, Collecting Tweets containing Bitcoin or BTC\n","tweets_path = 'data/sources/tweets_historical.csv'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import features.price_features as price_features\n","\n","indicators = ['rsi', 'macd']\n","basic_args = {'append': True, 'ewm': True, 'adjust': True, 'freq': freq, 'signal_indicators': True}\n","args = dict(zip(indicators, [basic_args] * len(indicators)))\n","\n","# args['rsi']['xa'] = 70\n","# args['rsi']['xb'] = 30\n","\n","prices_df, ranges_dict = price_features.main(\n","    prices_path=prices_path,\n","    ranges_dict_path=ranges_dict_path,\n","    save_path=save_path,\n","    onlyRead=False,\n","    freq=[freq, 'd'],\n","    timestamp_col='Timestamp',\n","    cleanNans=True,\n","    start_date=start_date,\n","    args=args\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from features.tweets_preprocess import (\n","    tweetsPreprocess,\n","    VADER_COLUMNS,\n","    TEXTBLOB_COLUMNS,\n",")\n","\n","# TODO: Save tweets sentiment independent of prices and one file per date range and frequency\n","\n","sentiment_cols = VADER_COLUMNS + TEXTBLOB_COLUMNS\n","\n","save_path='data/preprocess/twitter.csv'\n","\n","partial_file = os.path.splitext(save_path)\n","save_final_path = f'{partial_file[0]}_{start_date}_-_{end_date}{partial_file[1]}'\n","\n","if os.path.exists(save_final_path):\n","    tweets_df = pd.read_csv(save_final_path, sep='\\t', index_col='timestamp')\n","\n","    tweets_df = tweets_df.set_index(\n","        pd.to_datetime(tweets_df.index)\n","    )\n","\n","else:\n","    print(\"Start tweetsPreprocess\")\n","    tweets_df = tweetsPreprocess(\n","        tweets_path,\n","        freq=freq,\n","        sentiment_cols=sentiment_cols,\n","        # sentiment_cols=['Compound', 'Polarity'],\n","        aggregate_cols=['replies', 'likes', 'retweets'], # TODO: Also by volume of tweets??\n","        start_date=start_date,\n","        end_date=end_date,\n","        nrows=None,\n","        chunksize=5e5,\n","        save_path='data/preprocess/twitter.csv',\n","        write_files=False\n","    )\n","\n","remove_cols = [\n","    'replies_sum',\n","    'replies_mean',\n","    'likes_sum',\n","    'likes_mean',\n","    'retweets_sum',\n","    'retweets_mean',\n","]\n","tweets_df = tweets_df.drop(remove_cols, axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data = prices_df.merge(tweets_df, how='left', left_index=True, right_index=True)\n","data = data.reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["FEATURE_COLUMNS = []\n","for key in ranges_dict:\n","    FEATURE_COLUMNS += ranges_dict[key]['cols'] if ranges_dict[key]['normalize'] else []\n","\n","FEATURE_COLUMNS += list(tweets_df.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["signal_columns = prices_df.columns.str.contains('_XA_') | prices_df.columns.str.contains('_XB_') | prices_df.columns.str.contains('_A_') | prices_df.columns.str.contains('_B_')\n","FEATURE_COLUMNS = list(prices_df.columns[signal_columns])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if USE_PRICE_RANGE_COLUMNS:\n","\n","    diff_cols = len(ranges_dict['prices']['cols']) - len(FEATURE_COLUMNS) - int(POSITION_AS_OBSERVATION)\n","    print(f'Difference of {diff_cols} columns between prices cols and normalized cols')\n","    print('In order to use Group Normalization Layer with 2 groups, both groups should be equal and sorted to be one first and then the other.')\n","\n","    if diff_cols > 0:\n","        remove_cols = ['LR_14']\n","        print(f'The following columns are going to be removed: {remove_cols}')\n","        prices_cols = [col for col in ranges_dict['prices']['cols'] if col not in remove_cols]\n","    else:\n","        prices_cols = ranges_dict['prices']['cols']\n","\n","    # Add prices cols into the FEATURE_COLUMNS\n","    FEATURE_COLUMNS = prices_cols + FEATURE_COLUMNS\n","\n","# Make sure that PRICE_COL is in data\n","ALL_COLS = [PRICE_COLUMN] if PRICE_COLUMN not in FEATURE_COLUMNS else []\n","ALL_COLS += FEATURE_COLUMNS\n","\n","# Set the columns used in data PRICE_COL + FEATURE_COLS\n","data = data[ALL_COLS]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["assert not np.isinf(data).any(1).any(), data[np.isinf(data).any(1)]\n","assert not data.isnull().any().any()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# unit_factor = 60*24*30 # months \n","unit_factor = 24*30*12 # years \n","print(f'Data for {len(data.index) / unit_factor:.3f} units')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_time = 2\n","gap_time = 1/12\n","valid_time = (len(data.index) / unit_factor - train_time - 2 * gap_time) / 2\n","test_time = (len(data.index) / unit_factor - train_time - 2 * gap_time) / 2\n","\n","train_end = int(train_time * unit_factor)\n","valid_start = train_end + int(gap_time * unit_factor)\n","valid_end = valid_start + int(valid_time * unit_factor)\n","test_start = valid_end + int(gap_time * unit_factor)\n","test_end = test_start + int(test_time * unit_factor)\n","\n","train = data.iloc[0:train_end, :]\n","valid = data.iloc[valid_start:valid_end, :]\n","test = data.iloc[test_start:test_end, :]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from own_stock_env import OwnStocksEnv, REVENUE_REWARD, PRICE_REWARD\n","\n","# TODO: Steps scheduling, starting from low number of steps to high\n","# steps_schedule = [5, 10, 15, 20]\n","steps_schedule = [48]\n","steps_per_episode = steps_schedule[0] # 20\n","window_size = 1\n","POSITION_AS_OBSERVATION = True\n","CONSTANT_STEP = False\n","\n","num_parallel_environments = 1\n","\n","reward_type = REVENUE_REWARD\n","max_step_reward = 0\n","max_final_reward = 1\n","\n","SEED = 12345\n","\n","#### ONLY FOR TESTING OVERFITING\n","\n","# steps_per_episode = 5\n","# factor = 2\n","# # factor = 20\n","# train = train[0:steps_per_episode*factor]\n","# valid = valid[0:steps_per_episode*factor]\n","# test = test[0:steps_per_episode*factor]\n","\n","##############################################"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from utils import generateSplitEnvs\n","\n","tf_env, eval_tf_env, test_tf_env = generateSplitEnvs(\n","    train,\n","    valid,\n","    test,\n","    window_size,\n","    steps_per_episode,\n","    FEATURE_COLUMNS,\n","    reward_type=reward_type,\n","    max_final_reward=max_final_reward,\n","    max_step_reward=max_step_reward,\n","    num_parallel_environments=num_parallel_environments,\n","    position_as_observation=POSITION_AS_OBSERVATION,\n","    constant_step=CONSTANT_STEP,\n","    is_training=True,\n","    seed=SEED,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from absl import logging\n","# Added in last versions\n","# import tf_agents.system import multiprocessing\n","\n","logging.set_verbosity(logging.INFO)\n","# tf.logging.set_verbosity(tf.logging.INFO)\n","tf.compat.v1.enable_v2_behavior()\n","\n","# Added in last versions\n","# multiprocessing.enable_interactive_mode()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["agent = 'PPO'\n","\n","STEP = 'step'\n","EPISODE = 'episode'\n","agent_unit = {\n","    'DQN': STEP,\n","    'PPO': EPISODE,\n","    'REINFORCE': EPISODE,\n","}\n","unit = agent_unit[agent]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tensorflow.keras.optimizers import Adam, SGD\n","from tf_agents.utils import common\n","\n","# Params for train\n","num_iterations = 1000000\n","\n","summary_frequency = 100000\n","\n","train_steps_per_iteration = 1\n","collect_per_iteration = 3 * num_parallel_environments\n","replay_buffer_capacity = steps_per_episode * collect_per_iteration // num_parallel_environments + 1\n","\n","# TODO: Improve learning rate with schedule and on e-greedy too\n","batch_size = 32\n","learning_rate = 6e-5 # 3e-4\n","optimizer = Adam(learning_rate=learning_rate) # SGD(learning_rate=learning_rate) # Adam(learning_rate=learning_rate)\n","gradient_clipping = 5\n","\n","if agent == 'DQN':\n","    # TODO: Use other kind of policy like Boltzam?\n","    epsilon_greedy = 0.1\n","\n","    target_update_tau = 0.05\n","    target_update_period = 5\n","\n","    initial_collect_steps = num_iterations // 1000 # 1000\n","\n","    n_step_update = 1\n","\n","    td_errors_loss_fn = common.element_wise_huber_loss # common.element_wise_squared_loss # common.element_wise_huber_loss\n","\n","    gamma = 0.99\n","    reward_scale_factor = 1.0\n","\n","elif agent == 'PPO':\n","    \n","    importance_ratio_clipping = 0.2\n","    \n","    kl_cutoff_factor = 0 # 2.0\n","    kl_cutoff_coef = 1000.0\n","    initial_adaptive_kl_beta = 0 # 1.0\n","    adaptive_kl_target = 0.01\n","    adaptive_kl_tolerance = 0.3\n","\n","    normalize_observations=True\n","    normalize_rewards=True\n","    reward_norm_clipping=10.0 # Not used if normalize_rewards=False\n","    use_gae=True\n","    lambda_value=1 # 0.95 \n","    discount_factor=1 # TODO: Rethink on how to implement discount factor because reward by prices is accumulative\n","\n","    entropy_regularization = 0\n","    policy_l2_reg = 0\n","    value_function_l2_reg = 0\n","    shared_vars_l2_reg = 0\n","    value_pred_loss_coef = 0.5\n","    use_td_lambda_return = False\n","    log_prob_clipping = 0.0\n","    value_clipping = None\n","    num_epochs = 25\n","\n","use_tf_functions = True\n","\n","# Params for summaries and logging\n","log_interval = num_iterations // summary_frequency\n","log_interval = max(log_interval, steps_per_episode)\n","summaries_flush_secs = 10\n","summary_interval = num_iterations // summary_frequency\n","summary_interval = max(summary_interval, steps_per_episode)\n","debug_summaries = True\n","summarize_grads_and_vars = True\n","check_numerics = True\n","\n","# Params for eval\n","num_eval_episodes = eval_tf_env.envs[0].frame_bound[-1] // eval_tf_env.envs[0].steps_per_episode\n","num_eval_seeds = 1\n","eval_interval = summary_interval * 1\n","eval_interval = max(eval_interval, steps_per_episode)\n","eval_metrics_callback = None\n","\n","# Params for checkpoints\n","train_checkpoint_interval = eval_interval * 4\n","policy_checkpoint_interval = eval_interval * 1\n","rb_checkpoint_interval = eval_interval * 8"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# !rmdir /s /q .\\\\logs\\\\dqn"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["TRAIN_MODEL = True\n","\n","root_dir = 'logs\\\\' + agent\n","\n","if TRAIN_MODEL:\n","    root_dir = os.path.join(root_dir, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","else:\n","    root_dir = os.path.join(root_dir, '20200503-095638')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tf_agents.metrics import tf_metrics\n","\n","from utils import AgentEarlyStopping\n","\n","root_dir = os.path.expanduser(root_dir)\n","train_dir = os.path.join(root_dir, 'train')\n","eval_dir = os.path.join(root_dir, 'eval')\n","saved_model_dir = os.path.join(root_dir, 'policy_saved_model')\n","\n","train_summary_writer = tf.summary.create_file_writer(\n","    train_dir, flush_millis=summaries_flush_secs * 1000)\n","train_summary_writer.set_as_default()\n","\n","step_metrics = []\n","train_metrics = step_metrics + [\n","    # tf_metrics.NumberOfEpisodes(),\n","    # tf_metrics.EnvironmentSteps(),\n","    tf_metrics.AverageReturnMetric(batch_size=num_parallel_environments),\n","    # tf_metrics.AverageEpisodeLengthMetric(),\n","    # tf_metrics.ChosenActionHistogram(dtype=tf.int32),\n","]\n","\n","eval_summary_writer = tf.summary.create_file_writer(\n","    eval_dir, flush_millis=summaries_flush_secs * 1000)\n","eval_metrics = [\n","    tf_metrics.AverageReturnMetric(buffer_size=num_eval_episodes),\n","    # tf_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes)\n","]\n","eval_metrics_callback = AgentEarlyStopping(\n","    monitor='AverageReturn', min_delta=0.0001, patience=np.inf, patience_after_change=np.inf, verbose=1, mode='max'\n",")\n","\n","global_step = tf.compat.v1.train.get_or_create_global_step()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define Q-network\n","\n","train_sequence_length = window_size\n","\n","# dropout_layer = (0.2,0.2,0.2,0.2,0.2)\n","dropout_layer = None\n","activation_fn = tf.nn.leaky_relu # tf.keras.activations.relu # tf.keras.activations.tanh\n","\n","if agent == 'DQN':\n","    if train_sequence_length > 1:\n","        input_fc_layer_params = (8,)\n","        lstm_size = (16,)\n","        output_fc_layer_params = (8,)\n","        \n","    else:\n","        fc_layer_params = (100,)\n","\n","elif agent == 'PPO':\n","    if train_sequence_length > 1:\n","        actor_fc_layers = (32,64,128,64,32)\n","        actor_lstm_size = (32,64,32,)\n","        actor_output_fc_layer = (32,64,32,)\n","        \n","        value_fc_layers = (32,64,32,)\n","        value_lstm_size = (16,)\n","        value_output_fc_layers = (32,64,32,)\n","    else:\n","        actor_fc_layers = (512,1024,2048,1024,512,)\n","        \n","        value_fc_layers = (512,1024,2048,1024,512,)\n","\n","\n","if agent == 'DQN':\n","    from tf_agents.networks import q_network\n","    from tf_agents.networks import q_rnn_network\n","\n","    if train_sequence_length > 1:\n","        q_net = q_rnn_network.QRnnNetwork(\n","            tf_env.observation_spec(),\n","            tf_env.action_spec(),\n","            input_fc_layer_params=input_fc_layer_params,\n","            lstm_size=lstm_size,\n","            output_fc_layer_params=output_fc_layer_params\n","        )\n","    else:\n","        q_net = q_network.QNetwork(\n","            tf_env.observation_spec(),\n","            tf_env.action_spec(),\n","            fc_layer_params=fc_layer_params,\n","            dropout_layer_params=dropout_layer,\n","        )\n","        train_sequence_length = n_step_update\n","\n","    if train_sequence_length != 1 and n_step_update != 1:\n","        raise NotImplementedError(\n","            'Currently not supporting n-step updates with stateful networks (i.e., RNNs)')\n","\n","elif agent == 'PPO':\n","    from tf_agents.networks import actor_distribution_network\n","    from tf_agents.networks import actor_distribution_rnn_network\n","    from tf_agents.networks import value_network\n","    from tf_agents.networks import value_rnn_network\n","\n","    if train_sequence_length > 1:\n","        actor_net = actor_distribution_rnn_network.ActorDistributionRnnNetwork(\n","            tf_env.observation_spec(),\n","            tf_env.action_spec(),\n","            input_fc_layer_params=actor_fc_layers,\n","            input_dropout_layer_params=dropout_layer,\n","            lstm_size=actor_lstm_size,\n","            activation_fn=activation_fn,\n","            output_fc_layer_params=actor_output_fc_layer)\n","        value_net = value_rnn_network.ValueRnnNetwork(\n","            tf_env.observation_spec(),\n","            input_fc_layer_params=value_fc_layers,\n","            input_dropout_layer_params=dropout_layer,\n","            lstm_size=value_lstm_size,\n","            activation_fn=activation_fn, # alredy relu on source code\n","            output_fc_layer_params=actor_output_fc_layer)\n","    else:\n","        actor_net = actor_distribution_network.ActorDistributionNetwork(\n","            tf_env.observation_spec(),\n","            tf_env.action_spec(),\n","            fc_layer_params=actor_fc_layers,\n","            dropout_layer_params=dropout_layer,\n","            activation_fn=activation_fn)\n","        value_net = value_network.ValueNetwork(\n","            tf_env.observation_spec(),\n","            fc_layer_params=value_fc_layers,\n","            dropout_layer_params=dropout_layer,\n","            activation_fn=activation_fn)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# TODO: Adapt for using step or episodes as unit to then can switch easily between TF-Agents\n","# Compare here: https://github.com/tensorflow/agents/blob/master/tf_agents/agents/ppo/examples/v2/train_eval_clip_agent.py\n","from tf_agents.agents.dqn import dqn_agent\n","from tf_agents.agents.ppo import ppo_agent # TODO: Use ppo_clip_agent which is the proposed above\n","from tf_agents.drivers import dynamic_step_driver, dynamic_episode_driver\n","from tf_agents.eval import metric_utils\n","from tf_agents.policies import random_tf_policy\n","from tf_agents.replay_buffers import tf_uniform_replay_buffer\n","from tf_agents.utils import common\n","\n","from tf_agents.policies import policy_saver\n","\n","from tensorflow.compat.v2 import summary\n","from tensorflow import equal as tf_equal\n","\n","from utils import (\n","    train_eval,\n","    evaluate\n",")\n","\n","with summary.record_if(\n","    lambda: tf_equal(global_step % summary_interval, 0)):\n","\n","    if agent == 'DQN':\n","      # TODO(b/127301657): Decay epsilon based on global step, cf. cl/188907839\n","      tf_agent = dqn_agent.DqnAgent(\n","          tf_env.time_step_spec(),\n","          tf_env.action_spec(),\n","          q_network=q_net,\n","          epsilon_greedy=epsilon_greedy,\n","          n_step_update=n_step_update,\n","          target_update_tau=target_update_tau,\n","          target_update_period=target_update_period,\n","          optimizer=optimizer,\n","          td_errors_loss_fn=td_errors_loss_fn,\n","          gamma=gamma,\n","          reward_scale_factor=reward_scale_factor,\n","          gradient_clipping=gradient_clipping,\n","          debug_summaries=debug_summaries,\n","          summarize_grads_and_vars=summarize_grads_and_vars,\n","          check_numerics=check_numerics,\n","          train_step_counter=global_step)\n","    elif agent == 'PPO':\n","      # TODO: Use ppo_clip_agent which is the proposed above\n","      # tf_agent = ppo_clip_agent.PPOClipAgent(\n","      tf_agent = ppo_agent.PPOAgent(\n","        tf_env.time_step_spec(),\n","        tf_env.action_spec(),\n","        optimizer=optimizer,\n","        actor_net=actor_net,\n","        value_net=value_net,\n","        importance_ratio_clipping=importance_ratio_clipping,\n","        kl_cutoff_factor=kl_cutoff_factor,\n","        kl_cutoff_coef=kl_cutoff_coef,\n","        initial_adaptive_kl_beta=initial_adaptive_kl_beta,\n","        adaptive_kl_target=adaptive_kl_target,\n","        adaptive_kl_tolerance=adaptive_kl_tolerance,\n","        lambda_value=lambda_value,\n","        discount_factor=discount_factor,\n","        entropy_regularization=entropy_regularization,\n","        policy_l2_reg=policy_l2_reg,\n","        value_function_l2_reg=value_function_l2_reg,\n","        # shared_vars_l2_reg=shared_vars_l2_reg,\n","        value_pred_loss_coef=value_pred_loss_coef,\n","        normalize_observations=normalize_observations,\n","        use_gae=use_gae,\n","        use_td_lambda_return=use_td_lambda_return,\n","        normalize_rewards=normalize_rewards,\n","        reward_norm_clipping=reward_norm_clipping,\n","        log_prob_clipping=log_prob_clipping,\n","        gradient_clipping=gradient_clipping,\n","        # value_clipping=value_clipping,\n","        num_epochs=num_epochs,\n","        debug_summaries=debug_summaries,\n","        summarize_grads_and_vars=summarize_grads_and_vars,\n","        check_numerics=check_numerics,\n","        train_step_counter=global_step)\n","    else:\n","      raise NotImplementedError('Other agents than DQN and PPO are not yet implemented')\n","\n","    for steps_per_episode in steps_schedule:\n","\n","      logging.info(\n","        f'Steps per episode equal to {steps_per_episode}'\n","      )\n","\n","      tf_env, eval_tf_env, test_tf_env = generateSplitEnvs(\n","        train,\n","        valid,\n","        test,\n","        window_size,\n","        steps_per_episode,\n","        FEATURE_COLUMNS,\n","        reward_type=reward_type,\n","        max_final_reward=max_final_reward,\n","        max_step_reward=max_step_reward,\n","        num_parallel_environments=num_parallel_environments,\n","        position_as_observation=POSITION_AS_OBSERVATION,\n","        constant_step=False,\n","        is_training=True,\n","        seed=SEED,\n","      )\n","\n","      summary.scalar(\n","        name='step_scheduling', data=steps_per_episode, step=global_step)\n","\n","      eval_metrics_callback.reset()\n","\n","      train_eval(\n","        tf_agent,\n","        num_iterations,\n","        batch_size,\n","        tf_env,\n","        eval_tf_env,\n","        train_metrics,\n","        step_metrics,\n","        eval_metrics,\n","        global_step,\n","        replay_buffer_capacity,\n","        num_parallel_environments,\n","        collect_per_iteration,\n","        train_steps_per_iteration,\n","        train_dir,\n","        saved_model_dir,\n","        eval_summary_writer,\n","        num_eval_episodes,\n","        num_eval_seeds=num_eval_seeds,\n","        eval_metrics_callback=eval_metrics_callback,\n","        train_sequence_length=train_sequence_length,\n","        initial_collect_steps=initial_collect_steps if agent=='DQN' else None,\n","        log_interval=log_interval,\n","        eval_interval=eval_interval,\n","        policy_checkpoint_interval=policy_checkpoint_interval,\n","        train_checkpoint_interval=train_checkpoint_interval,\n","        rb_checkpoint_interval=rb_checkpoint_interval,\n","        train_model=TRAIN_MODEL,\n","        use_tf_functions=use_tf_functions,\n","        eval_early_stopping=True,\n","        seed=SEED\n","      )\n","\n","      summary.scalar(\n","        name='step_scheduling', data=steps_per_episode, step=global_step)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tensorflow.compat.v2 import summary"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# One last evaluation\n","results = evaluate(eval_metrics, eval_tf_env, tf_agent.policy, num_eval_episodes, num_eval_seeds, global_step, eval_summary_writer, summary_prefix='Metrics')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["env_data = train\n","\n","all_envs = {}\n","\n","full_env = OwnStocksEnv(\n","    df=env_data,\n","    window_size=window_size,\n","    frame_bound=(window_size, len(env_data)),\n","    steps_per_episode=len(env_data) - window_size, # steps_per_episode,\n","    constant_step=True,\n","    is_training=False,\n","    feature_columns=FEATURE_COLUMNS,\n","    position_as_observation=POSITION_AS_OBSERVATION,\n","    reward_type=reward_type,\n","    max_final_reward=max_final_reward,\n","    max_step_reward=max_step_reward,\n",")\n","all_envs['Full eval'] = full_env\n","\n","#TODO: For the is_training=True we have to make that all executions are using same cases\n","step_env = OwnStocksEnv(\n","    df=env_data,\n","    window_size=window_size,\n","    frame_bound=(window_size, len(env_data)),\n","    steps_per_episode=steps_per_episode,\n","    constant_step=True,\n","    is_training=True,\n","    feature_columns=FEATURE_COLUMNS,\n","    position_as_observation=POSITION_AS_OBSERVATION,\n","    reward_type=reward_type,\n","    max_final_reward=max_final_reward,\n","    max_step_reward=max_step_reward,\n",")\n","all_envs[f'Eval step of {steps_per_episode}'] = step_env\n","\n","large_step_env = OwnStocksEnv(\n","    df=env_data,\n","    window_size=window_size,\n","    frame_bound=(window_size, len(env_data)),\n","    steps_per_episode=10 * steps_per_episode,\n","    constant_step=True,\n","    is_training=True,\n","    feature_columns=FEATURE_COLUMNS,\n","    position_as_observation=POSITION_AS_OBSERVATION,\n","    reward_type=reward_type,\n","    max_final_reward=max_final_reward,\n","    max_step_reward=max_step_reward,\n",")\n","all_envs[f'Eval step of {10*steps_per_episode}'] = large_step_env\n","\n","if int(0.1 * steps_per_episode) > 1:\n","    small_step_env = OwnStocksEnv(\n","        df=env_data,\n","        window_size=window_size,\n","        frame_bound=(window_size, len(env_data)),\n","        steps_per_episode=int(0.1 * steps_per_episode),\n","        constant_step=True,\n","        is_training=True,\n","        feature_columns=FEATURE_COLUMNS,\n","        position_as_observation=POSITION_AS_OBSERVATION,\n","        reward_type=reward_type,\n","        max_final_reward=max_final_reward,\n","        max_step_reward=max_step_reward,\n","    )\n","    all_envs[f'Eval step of {int(0.1 * steps_per_episode)}'] = small_step_env"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from own_stock_env import runAllTestEnv"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend"]},"outputs":[],"source":["# Apply random policy on env\n","runAllTestEnv(all_envs, select_action_func=full_env.action_space.sample, deterministic_policy=False);"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Applying long term policy (buy at initial and do not sell) on env\n","from gym_anytrading.envs import Actions \n","\n","def always_buy_func():\n","    return  Actions.Buy.value\n","\n","runAllTestEnv(all_envs, select_action_func=always_buy_func);"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend"]},"outputs":[],"source":["# Applying baseline policy on env\n","# Manual policy used as baseline\n","from gym_anytrading.envs import Positions, Actions\n","\n","# rsi_col = 'RSI_14'\n","# rsi_col = 'RSI_336'\n","# rsi_index = full_env.feature_columns.index(rsi_col)\n","\n","# RSI usually is between 0 and 100, here is normalized between -1 and 1\n","# The baseline strategy is buy at 30 and sell at 70 otherwise hold\n","# def select_baseline_action(observation, rsi_thresh_buy=-0.6, rsi_thresh_sell=0.4, rsi_index=rsi_index):\n","#     # Use only last observation\n","#     obs = observation[-1]\n","\n","#     position_value = int(obs[-1])\n","#     rsi = obs[rsi_index]\n","\n","#     if position_value == Positions.Short.value and rsi <= rsi_thresh_buy:\n","#         action = Actions.Buy.value\n","#     elif position_value == Positions.Long.value and rsi >= rsi_thresh_sell:\n","#         action = Actions.Sell.value\n","#     else:\n","#         # Hold\n","#         # if it was in short remain in short because is selling\n","#         # if it was in long remain in long because is buying\n","#         action = position_value\n","    \n","#     return action\n","\n","# RSI usually is between 0 and 100, here is normalized between -1 and 1\n","# The baseline strategy is buy at 30 and sell at 70 otherwise hold\n","def select_baseline_action(observation, buy_index, sell_index, trend_index):\n","    # Use only last observation\n","    obs = observation[-1]\n","\n","    position_value = int(obs[-1])\n","    buy_signal = obs[buy_index]\n","    sell_signal = obs[sell_index]\n","    trend_signal = obs[trend_index]\n","\n","    if position_value == Positions.Short.value and buy_signal and not trend_signal:\n","        action = Actions.Buy.value\n","    elif position_value == Positions.Long.value and sell_signal and trend_signal:\n","        action = Actions.Sell.value\n","    else:\n","        # Hold\n","        # if it was in short remain in short because is selling\n","        # if it was in long remain in long because is buying\n","        action = position_value\n","    \n","    return action\n","\n","buy_col = 'RSI_14_B_20' + '_' + 'hour'\n","sell_col = 'RSI_14_A_80' + '_' + 'hour'\n","trend_col = 'MACD_12_26_9_A_0' + '_' + 'hour'\n","\n","runAllTestEnv(all_envs, select_action_func=select_baseline_action, use_observation=True, buy_index=full_env.feature_columns.index(buy_col), sell_index=full_env.feature_columns.index(sell_col), trend_index=full_env.feature_columns.index(trend_col));"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def select_TFEnv_action(TFEnv, policy, done, time_step=None, policy_state=None):\n","    \n","    action_step = policy.action(time_step, policy_state)\n","    # distribution_step = policy._distribution(  # pylint: disable=protected-access\n","    #     time_step, policy_state)\n","    # if distribution_step.action.log_prob(0) > distribution_step.action.log_prob(1):\n","    #     print(distribution_step)\n","    #     print(distribution_step.action.log_prob(0), distribution_step.action.log_prob(1))\n","\n","    # TODO(b/134487572): TF2 while_loop seems to either ignore\n","    # parallel_iterations or doesn't properly propagate control dependencies\n","    # from one step to the next. Without this dep, self.env.step() is called\n","    # in parallel.\n","    with tf.control_dependencies(tf.nest.flatten([time_step])):\n","        next_time_step = TFEnv.step(action_step.action)\n","\n","    policy_state = action_step.state\n","\n","    action = action_step.action.numpy()[0]\n","    # print(action)\n","\n","    done = next_time_step.discount.numpy()[0] == 0\n","    # if done:\n","    #     display(TFEnv.envs[0].max_possible_profit_df.iloc[-1,0])\n","    #     print(TFEnv.envs[0]._total_profit)\n","    #     print(TFEnv.envs[0].calculate_revenue_ratio())\n","\n","    return action, done, next_time_step, policy_state"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["all_tf_envs = {}\n","\n","for key, value in all_envs.items():\n","    all_tf_envs[key] = tf_py_environment.TFPyEnvironment(GymWrapper(value))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["runAllTestEnv(all_tf_envs, select_action_func=select_TFEnv_action, use_model=True, isTFEnv=True, policy=tf_agent.policy)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3},"version":"3.7.6-final"},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3,"kernelspec":{"name":"python37664bitcryptotraderconda69cc994ed1944dadbb053620b665a6b3","display_name":"Python 3.7.6 64-bit ('crypto_trader': conda)"}},"nbformat":4,"nbformat_minor":2}