{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import pandas_ta\n","import numpy as np\n"," \n","# from gym_anytrading.datasets import FOREX_EURUSD_1H_ASK, STOCKS_GOOGL\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import os\n","import time\n","import datetime\n","from six.moves import range\n","\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["PRICE_COLUMN = 'close'\n","USE_PRICE_RANGE_COLUMNS = False\n","\n","freq = 'h'\n","start_date='2016-01-02'\n","end_date='2019-03-28'\n","\n","ranges_dict_path = 'data\\\\ranges_dict.pickle'\n","save_path = f'.\\\\data\\\\featured_prices_{freq}_start_{start_date}.csv'\n","\n","# prices_path = '.\\\\data\\\\prices_freq-min_2019-01-01_2019-03-28.csv'\n","prices_path = '.\\\\data\\\\sources\\\\coinbaseUSD_1-min_data_2014-12-01_to_2019-01-09.csv'\n","\n","# Scrapped from twitters from 2016-01-01 to 2019-03-29, Collecting Tweets containing Bitcoin or BTC\n","tweets_path = 'data/sources/tweets_historical.csv'"]},{"cell_type":"code","execution_count":4,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stderr","text":"[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\Lluis\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     C:\\Users\\Lluis\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\nLoading data...\nFilling All Time data\nFilling NA data\nAggregating from min to h level\nAggregating from min to d level\nGenerating TA features...\n1 out of 2 featuresDropping 792 rows because of NaN values\n"}],"source":["import features.price_features as price_features\n","\n","indicators = ['rsi', 'macd']\n","basic_args = {'append': True, 'ewm': True, 'adjust': True, 'freq': freq, 'signal_indicators': True}\n","args = dict(zip(indicators, [basic_args] * len(indicators)))\n","\n","# args['rsi']['xa'] = 70\n","# args['rsi']['xb'] = 30\n","\n","prices_df, ranges_dict = price_features.main(\n","    prices_path=prices_path,\n","    ranges_dict_path=ranges_dict_path,\n","    save_path=save_path,\n","    onlyRead=False,\n","    freq=[freq, 'd'],\n","    timestamp_col='Timestamp',\n","    cleanNans=True,\n","    start_date=start_date,\n","    args=args\n",")"]},{"cell_type":"code","execution_count":9,"metadata":{"tags":[]},"outputs":[],"source":["from features.tweets_preprocess import (\n","    tweetsPreprocess,\n","    VADER_COLUMNS,\n","    TEXTBLOB_COLUMNS,\n",")\n","\n","# TODO: Save tweets sentiment independent of prices and one file per date range and frequency\n","\n","sentiment_cols = VADER_COLUMNS + TEXTBLOB_COLUMNS\n","\n","save_path='data/preprocess/twitter.csv'\n","\n","partial_file = os.path.splitext(save_path)\n","save_final_path = f'{partial_file[0]}_{start_date}_-_{end_date}{partial_file[1]}'\n","\n","if os.path.exists(save_final_path):\n","    tweets_df = pd.read_csv(save_final_path, sep='\\t', index_col='timestamp')\n","\n","    tweets_df = tweets_df.set_index(\n","        pd.to_datetime(tweets_df.index)\n","    )\n","\n","else:\n","    print(\"Start tweetsPreprocess\")\n","    tweets_df = tweetsPreprocess(\n","        tweets_path,\n","        freq=freq,\n","        sentiment_cols=sentiment_cols,\n","        # sentiment_cols=['Compound', 'Polarity'],\n","        aggregate_cols=['replies', 'likes', 'retweets'], # TODO: Also by volume of tweets??\n","        start_date=start_date,\n","        end_date=end_date,\n","        nrows=100000,\n","        chunksize=5e5,\n","        save_path='data/preprocess/twitter.csv',\n","        write_files=False\n","    )\n","\n","remove_cols = [\n","    'replies_sum',\n","    'replies_mean',\n","    'likes_sum',\n","    'likes_mean',\n","    'retweets_sum',\n","    'retweets_mean',\n","]\n","tweets_df = tweets_df.drop(remove_cols, axis=1)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["data = prices_df.merge(tweets_df, how='left', left_index=True, right_index=True)\n","data = data.reset_index(drop=True)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["FEATURE_COLUMNS = []\n","for key in ranges_dict:\n","    FEATURE_COLUMNS += ranges_dict[key]['cols'] if ranges_dict[key]['normalize'] else []\n","\n","FEATURE_COLUMNS += list(tweets_df.columns)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["signal_columns = prices_df.columns.str.contains('_XA_') | prices_df.columns.str.contains('_XB_') | prices_df.columns.str.contains('_A_') | prices_df.columns.str.contains('_B_')\n","FEATURE_COLUMNS = list(prices_df.columns[signal_columns])"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["if USE_PRICE_RANGE_COLUMNS:\n","\n","    diff_cols = len(ranges_dict['prices']['cols']) - len(FEATURE_COLUMNS) - int(POSITION_AS_OBSERVATION)\n","    print(f'Difference of {diff_cols} columns between prices cols and normalized cols')\n","    print('In order to use Group Normalization Layer with 2 groups, both groups should be equal and sorted to be one first and then the other.')\n","\n","    if diff_cols > 0:\n","        remove_cols = ['LR_14']\n","        print(f'The following columns are going to be removed: {remove_cols}')\n","        prices_cols = [col for col in ranges_dict['prices']['cols'] if col not in remove_cols]\n","    else:\n","        prices_cols = ranges_dict['prices']['cols']\n","\n","    # Add prices cols into the FEATURE_COLUMNS\n","    FEATURE_COLUMNS = prices_cols + FEATURE_COLUMNS\n","\n","# Make sure that PRICE_COL is in data\n","ALL_COLS = [PRICE_COLUMN] if PRICE_COLUMN not in FEATURE_COLUMNS else []\n","ALL_COLS += FEATURE_COLUMNS\n","\n","# Set the columns used in data PRICE_COL + FEATURE_COLS\n","data = data[ALL_COLS]"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["assert not np.isinf(data).any(1).any(), data[np.isinf(data).any(1)]\n","assert not data.isnull().any().any()"]},{"cell_type":"code","execution_count":15,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"Data for 2.969 units\n"}],"source":["# unit_factor = 60*24*30 # months \n","unit_factor = 24*30*12 # years \n","print(f'Data for {len(data.index) / unit_factor:.3f} units')"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["train_time = 2\n","gap_time = 1/12\n","valid_time = (len(data.index) / unit_factor - train_time - 2 * gap_time) / 2\n","test_time = (len(data.index) / unit_factor - train_time - 2 * gap_time) / 2\n","\n","train_end = int(train_time * unit_factor)\n","valid_start = train_end + int(gap_time * unit_factor)\n","valid_end = valid_start + int(valid_time * unit_factor)\n","test_start = valid_end + int(gap_time * unit_factor)\n","test_end = test_start + int(test_time * unit_factor)\n","\n","train = data.iloc[0:train_end, :]\n","valid = data.iloc[valid_start:valid_end, :]\n","test = data.iloc[test_start:test_end, :]"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["from RL_env.stock_env import RLStocksEnv, REVENUE_REWARD, PRICE_REWARD\n","\n","# TODO: Steps scheduling, starting from low number of steps to high\n","steps_schedule = [4, 8, 16, 32, 64, 128, 256]\n","steps_per_episode = 64 # steps_schedule[-1]\n","window_size = 1\n","POSITION_AS_OBSERVATION = True\n","CONSTANT_STEP = False\n","\n","num_parallel_environments = 1\n","\n","reward_type = REVENUE_REWARD\n","max_step_reward = 0\n","max_final_reward = 1\n","\n","SEED = 12345\n","\n","#### ONLY FOR TESTING OVERFITING\n","\n","# steps_per_episode = 5\n","# factor = 2\n","# # factor = 20\n","# train = train[0:steps_per_episode*factor]\n","# valid = valid[0:steps_per_episode*factor]\n","# test = test[0:steps_per_episode*factor]\n","\n","##############################################"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["from utils import generateSplitEnvs\n","\n","tf_env, eval_tf_env, test_tf_env = generateSplitEnvs(\n","    train,\n","    valid,\n","    test,\n","    window_size,\n","    steps_per_episode,\n","    FEATURE_COLUMNS,\n","    reward_type=reward_type,\n","    max_final_reward=max_final_reward,\n","    max_step_reward=max_step_reward,\n","    num_parallel_environments=num_parallel_environments,\n","    position_as_observation=POSITION_AS_OBSERVATION,\n","    constant_step=CONSTANT_STEP,\n","    is_training=True,\n","    seed=SEED,\n",")"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["from absl import logging\n","# Added in last versions\n","# import tf_agents.system import multiprocessing\n","\n","logging.set_verbosity(logging.INFO)\n","# tf.logging.set_verbosity(tf.logging.INFO)\n","tf.compat.v1.enable_v2_behavior()\n","\n","# Added in last versions\n","# multiprocessing.enable_interactive_mode()"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["agent = 'PPO'\n","\n","STEP = 'step'\n","EPISODE = 'episode'\n","agent_unit = {\n","    'DQN': STEP,\n","    'PPO': EPISODE,\n","    'REINFORCE': EPISODE,\n","}\n","unit = agent_unit[agent]"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["from tensorflow.keras.optimizers import Adam, SGD\n","from tf_agents.utils import common\n","\n","# Params for train\n","num_iterations = 10000000\n","# TODO: Adapt values to the step scheduling (replay_buffer_capacity, num_eval_episodes and intervals if too small)\n","\n","train_steps_per_iteration = 1\n","collect_per_iteration = 3 * num_parallel_environments\n","\n","# TODO: Improve learning rate with schedule and on e-greedy too\n","batch_size = 32\n","learning_rate = 6e-5 # 3e-4\n","optimizer = Adam(learning_rate=learning_rate) # SGD(learning_rate=learning_rate) # Adam(learning_rate=learning_rate)\n","gradient_clipping = 5\n","\n","if agent == 'DQN':\n","    # TODO: Use other kind of policy like Boltzam?\n","    epsilon_greedy = 0.1\n","\n","    target_update_tau = 0.05\n","    target_update_period = 5\n","\n","    initial_collect_steps = num_iterations // 1000 # 1000\n","\n","    n_step_update = 1\n","\n","    td_errors_loss_fn = common.element_wise_huber_loss # common.element_wise_squared_loss # common.element_wise_huber_loss\n","\n","    gamma = 0.99\n","    reward_scale_factor = 1.0\n","\n","elif agent == 'PPO':\n","    \n","    importance_ratio_clipping = 0.2\n","    \n","    kl_cutoff_factor = 0 # 2.0\n","    kl_cutoff_coef = 1000.0\n","    initial_adaptive_kl_beta = 0 # 1.0\n","    adaptive_kl_target = 0.01\n","    adaptive_kl_tolerance = 0.3\n","\n","    normalize_observations=True\n","    normalize_rewards=True\n","    reward_norm_clipping=10.0 # Not used if normalize_rewards=False\n","    use_gae=True\n","    lambda_value=1 # 0.95 \n","    discount_factor=1 # TODO: Rethink on how to implement discount factor because reward by prices is accumulative\n","\n","    entropy_regularization = 0\n","    policy_l2_reg = 0\n","    value_function_l2_reg = 0\n","    shared_vars_l2_reg = 0\n","    value_pred_loss_coef = 0.5\n","    use_td_lambda_return = False\n","    log_prob_clipping = 0.0\n","    value_clipping = None\n","    num_epochs = 25\n","\n","use_tf_functions = True\n","\n","# Params for summaries and logging\n","summary_interval = 400\n","summaries_flush_secs = 10\n","summary_interval = max(summary_interval, steps_per_episode)\n","log_interval = summary_interval * 1\n","\n","debug_summaries = True\n","summarize_grads_and_vars = True\n","check_numerics = False\n","\n","# Params for eval\n","num_eval_seeds = 1\n","eval_interval = summary_interval * 2\n","\n","# Params for checkpoints\n","train_checkpoint_interval = eval_interval * 20\n","policy_checkpoint_interval = eval_interval * 10\n","rb_checkpoint_interval = eval_interval * 40"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["# !rmdir /s /q .\\\\logs\\\\dqn"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["TRAIN_MODEL = True\n","\n","root_dir = 'logs\\\\' + agent\n","\n","if TRAIN_MODEL:\n","    root_dir = os.path.join(root_dir, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","else:\n","    root_dir = os.path.join(root_dir, '20200529-095708')"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["from tf_agents.metrics import tf_metrics\n","\n","from utils import AgentEarlyStopping\n","\n","root_dir = os.path.expanduser(root_dir)\n","train_dir = os.path.join(root_dir, 'train')\n","eval_dir = os.path.join(root_dir, 'eval')\n","saved_model_dir = os.path.join(root_dir, 'policy_saved_model')\n","\n","train_summary_writer = tf.summary.create_file_writer(\n","    train_dir, flush_millis=summaries_flush_secs * 1000)\n","train_summary_writer.set_as_default()\n","\n","step_metrics = []\n","train_metrics = step_metrics + [\n","    # tf_metrics.NumberOfEpisodes(),\n","    # tf_metrics.EnvironmentSteps(),\n","    tf_metrics.AverageReturnMetric(batch_size=num_parallel_environments),\n","    # tf_metrics.AverageEpisodeLengthMetric(),\n","    # tf_metrics.ChosenActionHistogram(dtype=tf.int32),\n","]\n","\n","eval_summary_writer = tf.summary.create_file_writer(\n","    eval_dir, flush_millis=summaries_flush_secs * 1000)\n","\n","eval_metrics = [\n","    tf_metrics.AverageReturnMetric(buffer_size=1),\n","    # tf_metrics.AverageEpisodeLengthMetric(buffer_size=1)\n","]\n","\n","eval_metrics_callback = AgentEarlyStopping(\n","    monitor='AverageReturn', min_delta=0.0001, patience=15, warmup=45, verbose=1, mode='max'\n",")\n","\n","global_step = tf.compat.v1.train.get_or_create_global_step()"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["# Define Q-network\n","\n","train_sequence_length = window_size\n","\n","# dropout_layer = (0.2,0.2,0.2,0.2,0.2)\n","dropout_layer = None\n","activation_fn = tf.nn.leaky_relu # tf.keras.activations.relu # tf.keras.activations.tanh\n","\n","if agent == 'DQN':\n","    if train_sequence_length > 1:\n","        input_fc_layer_params = (8,)\n","        lstm_size = (16,)\n","        output_fc_layer_params = (8,)\n","        \n","    else:\n","        fc_layer_params = (100,)\n","\n","elif agent == 'PPO':\n","    if train_sequence_length > 1:\n","        actor_fc_layers = (32,64,128,64,32)\n","        actor_lstm_size = (32,64,32,)\n","        actor_output_fc_layer = (32,64,32,)\n","        \n","        value_fc_layers = (32,64,32,)\n","        value_lstm_size = (16,)\n","        value_output_fc_layers = (32,64,32,)\n","    else:\n","        actor_fc_layers = (512,1024,2048,1024,512,)\n","        \n","        value_fc_layers = (512,1024,2048,1024,512,)\n","\n","\n","if agent == 'DQN':\n","    from tf_agents.networks import q_network\n","    from tf_agents.networks import q_rnn_network\n","\n","    if train_sequence_length > 1:\n","        q_net = q_rnn_network.QRnnNetwork(\n","            tf_env.observation_spec(),\n","            tf_env.action_spec(),\n","            input_fc_layer_params=input_fc_layer_params,\n","            lstm_size=lstm_size,\n","            output_fc_layer_params=output_fc_layer_params\n","        )\n","    else:\n","        q_net = q_network.QNetwork(\n","            tf_env.observation_spec(),\n","            tf_env.action_spec(),\n","            fc_layer_params=fc_layer_params,\n","            dropout_layer_params=dropout_layer,\n","        )\n","        train_sequence_length = n_step_update\n","\n","    if train_sequence_length != 1 and n_step_update != 1:\n","        raise NotImplementedError(\n","            'Currently not supporting n-step updates with stateful networks (i.e., RNNs)')\n","\n","elif agent == 'PPO':\n","    from tf_agents.networks import actor_distribution_network\n","    from tf_agents.networks import actor_distribution_rnn_network\n","    from tf_agents.networks import value_network\n","    from tf_agents.networks import value_rnn_network\n","\n","    if train_sequence_length > 1:\n","        actor_net = actor_distribution_rnn_network.ActorDistributionRnnNetwork(\n","            tf_env.observation_spec(),\n","            tf_env.action_spec(),\n","            input_fc_layer_params=actor_fc_layers,\n","            input_dropout_layer_params=dropout_layer,\n","            lstm_size=actor_lstm_size,\n","            activation_fn=activation_fn,\n","            output_fc_layer_params=actor_output_fc_layer)\n","        value_net = value_rnn_network.ValueRnnNetwork(\n","            tf_env.observation_spec(),\n","            input_fc_layer_params=value_fc_layers,\n","            input_dropout_layer_params=dropout_layer,\n","            lstm_size=value_lstm_size,\n","            activation_fn=activation_fn, # alredy relu on source code\n","            output_fc_layer_params=actor_output_fc_layer)\n","    else:\n","        actor_net = actor_distribution_network.ActorDistributionNetwork(\n","            tf_env.observation_spec(),\n","            tf_env.action_spec(),\n","            fc_layer_params=actor_fc_layers,\n","            dropout_layer_params=dropout_layer,\n","            activation_fn=activation_fn)\n","        value_net = value_network.ValueNetwork(\n","            tf_env.observation_spec(),\n","            fc_layer_params=value_fc_layers,\n","            dropout_layer_params=dropout_layer,\n","            activation_fn=activation_fn)"]},{"cell_type":"code","execution_count":28,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stderr","text":"INFO:absl:Steps per episode equal to 4\nINFO:absl:No checkpoint available at logs\\PPO\\20200709-084133\\train\nINFO:absl:No checkpoint available at logs\\PPO\\20200709-084133\\train\\policy\nINFO:absl:Initial eval metric\nINFO:absl: \n\t\t AverageReturn = 0.0\nINFO:absl:Starting training...\nINFO:absl:step = 400, loss = 0.212587\nINFO:absl:3.490 steps/sec\nINFO:absl:collect_time = 1.181, train_time = 113.358, summary_time = 0.076\nINFO:absl:step = 800, loss = 0.936127\nINFO:absl:39.526 steps/sec\nINFO:absl:collect_time = 0.689, train_time = 9.404, summary_time = 0.027\nINFO:absl: \n\t\t AverageReturn = 0.0\nINFO:absl:Calculate Evaluation lasts 13.521 s\nINFO:absl:step = 1200, loss = 0.044658\nINFO:absl:38.680 steps/sec\nINFO:absl:collect_time = 0.749, train_time = 9.559, summary_time = 0.033\nINFO:absl:step = 1600, loss = 5.457369\nINFO:absl:38.038 steps/sec\nINFO:absl:collect_time = 0.766, train_time = 9.711, summary_time = 0.039\nINFO:absl: \n\t\t AverageReturn = 0.0\nINFO:absl:Calculate Evaluation lasts 14.636 s\nINFO:absl:step = 2000, loss = 0.286236\nINFO:absl:36.448 steps/sec\nINFO:absl:collect_time = 0.828, train_time = 10.114, summary_time = 0.033\nINFO:absl:step = 2400, loss = 0.409718\nINFO:absl:37.140 steps/sec\nINFO:absl:collect_time = 0.779, train_time = 9.954, summary_time = 0.037\nINFO:absl: \n\t\t AverageReturn = 0.0\nINFO:absl:Calculate Evaluation lasts 14.686 s\nINFO:absl:step = 2800, loss = 8.772027\nINFO:absl:37.638 steps/sec\nINFO:absl:collect_time = 0.777, train_time = 9.819, summary_time = 0.032\nINFO:absl:step = 3200, loss = 2.488443\nINFO:absl:37.434 steps/sec\nINFO:absl:collect_time = 0.805, train_time = 9.845, summary_time = 0.036\nINFO:absl: \n\t\t AverageReturn = 0.0\nINFO:absl:Calculate Evaluation lasts 15.857 s\n"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m<ipython-input-28-0639037b034e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0muse_tf_functions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_tf_functions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m         \u001b[0meval_early_stopping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m         \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSEED\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m       )\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Users\\Lluis\\Desktop\\Machine Learning\\Crypto_trader\\utils.py\u001b[0m in \u001b[0;36mtrain_eval\u001b[1;34m(tf_agent, num_iterations, batch_size, tf_env, eval_tf_env, train_metrics, step_metrics, eval_metrics, global_step, steps_per_episode, num_parallel_environments, collect_per_iteration, train_steps_per_iteration, train_dir, saved_model_dir, eval_summary_writer, num_eval_episodes, num_eval_seeds, eval_metrics_callback, train_sequence_length, initial_collect_steps, log_interval, eval_interval, policy_checkpoint_interval, train_checkpoint_interval, rb_checkpoint_interval, train_model, use_tf_functions, eval_early_stopping, seed)\u001b[0m\n\u001b[0;32m    415\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_steps_per_iteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m                 \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    418\u001b[0m             \u001b[0mtrain_time\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\envs\\crypto_trader\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\envs\\crypto_trader\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    604\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n","\u001b[1;32m~\\Anaconda3\\envs\\crypto_trader\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\envs\\crypto_trader\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\envs\\crypto_trader\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\envs\\crypto_trader\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[1;32m~\\Anaconda3\\envs\\crypto_trader\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# TODO: Adapt for using step or episodes as unit to then can switch easily between TF-Agents\n","# Compare here: https://github.com/tensorflow/agents/blob/master/tf_agents/agents/ppo/examples/v2/train_eval_clip_agent.py\n","from tf_agents.agents.dqn import dqn_agent\n","from tf_agents.agents.ppo import ppo_agent # TODO: Use ppo_clip_agent which is the proposed above\n","from tf_agents.drivers import dynamic_step_driver, dynamic_episode_driver\n","from tf_agents.eval import metric_utils\n","from tf_agents.policies import random_tf_policy\n","from tf_agents.replay_buffers import tf_uniform_replay_buffer\n","from tf_agents.utils import common\n","\n","from tf_agents.policies import policy_saver\n","\n","from tensorflow.compat.v2 import summary\n","from tensorflow import equal as tf_equal\n","\n","from utils import (\n","    train_eval,\n","    evaluate\n",")\n","\n","with summary.record_if(\n","    lambda: tf_equal(global_step % summary_interval, 0)):\n","\n","    if agent == 'DQN':\n","      # TODO(b/127301657): Decay epsilon based on global step, cf. cl/188907839\n","      tf_agent = dqn_agent.DqnAgent(\n","          tf_env.time_step_spec(),\n","          tf_env.action_spec(),\n","          q_network=q_net,\n","          epsilon_greedy=epsilon_greedy,\n","          n_step_update=n_step_update,\n","          target_update_tau=target_update_tau,\n","          target_update_period=target_update_period,\n","          optimizer=optimizer,\n","          td_errors_loss_fn=td_errors_loss_fn,\n","          gamma=gamma,\n","          reward_scale_factor=reward_scale_factor,\n","          gradient_clipping=gradient_clipping,\n","          debug_summaries=debug_summaries,\n","          summarize_grads_and_vars=summarize_grads_and_vars,\n","          check_numerics=check_numerics,\n","          train_step_counter=global_step)\n","    elif agent == 'PPO':\n","      # TODO: Use ppo_clip_agent which is the proposed above\n","      # tf_agent = ppo_clip_agent.PPOClipAgent(\n","      tf_agent = ppo_agent.PPOAgent(\n","        tf_env.time_step_spec(),\n","        tf_env.action_spec(),\n","        optimizer=optimizer,\n","        actor_net=actor_net,\n","        value_net=value_net,\n","        importance_ratio_clipping=importance_ratio_clipping,\n","        kl_cutoff_factor=kl_cutoff_factor,\n","        kl_cutoff_coef=kl_cutoff_coef,\n","        initial_adaptive_kl_beta=initial_adaptive_kl_beta,\n","        adaptive_kl_target=adaptive_kl_target,\n","        adaptive_kl_tolerance=adaptive_kl_tolerance,\n","        lambda_value=lambda_value,\n","        discount_factor=discount_factor,\n","        entropy_regularization=entropy_regularization,\n","        policy_l2_reg=policy_l2_reg,\n","        value_function_l2_reg=value_function_l2_reg,\n","        # shared_vars_l2_reg=shared_vars_l2_reg,\n","        value_pred_loss_coef=value_pred_loss_coef,\n","        normalize_observations=normalize_observations,\n","        use_gae=use_gae,\n","        use_td_lambda_return=use_td_lambda_return,\n","        normalize_rewards=normalize_rewards,\n","        reward_norm_clipping=reward_norm_clipping,\n","        log_prob_clipping=log_prob_clipping,\n","        gradient_clipping=gradient_clipping,\n","        # value_clipping=value_clipping,\n","        num_epochs=num_epochs,\n","        debug_summaries=debug_summaries,\n","        summarize_grads_and_vars=summarize_grads_and_vars,\n","        check_numerics=check_numerics,\n","        train_step_counter=global_step)\n","    else:\n","      raise NotImplementedError('Other agents than DQN and PPO are not yet implemented')\n","\n","    policy_loaded = False\n","    for steps_per_episode in steps_schedule:\n","\n","      if not TRAIN_MODEL and policy_loaded:\n","          break\n","\n","      logging.info(\n","        f'Steps per episode equal to {steps_per_episode}'\n","      )\n","\n","      tf_env, eval_tf_env, test_tf_env = generateSplitEnvs(\n","        train,\n","        valid,\n","        test,\n","        window_size,\n","        steps_per_episode,\n","        FEATURE_COLUMNS,\n","        reward_type=reward_type,\n","        max_final_reward=max_final_reward,\n","        max_step_reward=max_step_reward,\n","        num_parallel_environments=num_parallel_environments,\n","        position_as_observation=POSITION_AS_OBSERVATION,\n","        constant_step=False,\n","        is_training=True,\n","        seed=SEED,\n","      )\n","\n","      num_eval_episodes = eval_tf_env.envs[0].frame_bound[-1] // eval_tf_env.envs[0].steps_per_episode\n","      for metric in eval_metrics:\n","          metric.batch_size = num_eval_episodes\n","\n","      summary.scalar(\n","        name='step_scheduling', data=steps_per_episode, step=global_step)\n","\n","      eval_metrics_callback.reset()\n","\n","      train_eval(\n","        tf_agent,\n","        num_iterations,\n","        batch_size,\n","        tf_env,\n","        eval_tf_env,\n","        train_metrics,\n","        step_metrics,\n","        eval_metrics,\n","        global_step,\n","        steps_per_episode,\n","        num_parallel_environments,\n","        collect_per_iteration,\n","        train_steps_per_iteration,\n","        train_dir,\n","        saved_model_dir,\n","        eval_summary_writer,\n","        num_eval_episodes,\n","        num_eval_seeds=num_eval_seeds,\n","        eval_metrics_callback=eval_metrics_callback,\n","        train_sequence_length=train_sequence_length,\n","        initial_collect_steps=initial_collect_steps if agent=='DQN' else None,\n","        log_interval=log_interval,\n","        eval_interval=eval_interval,\n","        policy_checkpoint_interval=policy_checkpoint_interval,\n","        train_checkpoint_interval=train_checkpoint_interval,\n","        rb_checkpoint_interval=rb_checkpoint_interval,\n","        train_model=TRAIN_MODEL,\n","        use_tf_functions=use_tf_functions,\n","        eval_early_stopping=True,\n","        seed=SEED\n","      )\n","\n","      policy_loaded = True\n","\n","      summary.scalar(\n","        name='step_scheduling', data=steps_per_episode, step=global_step)"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"output_type":"stream","name":"stderr","text":"INFO:absl:Checkpoint available: logs\\PPO\\20200529-095708\\train\\policy\\ckpt-344000\n"},{"output_type":"execute_result","data":{"text/plain":"<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1fa8f0ade48>"},"metadata":{},"execution_count":28}],"source":["from tf_agents.utils import common\n","\n","best_step = 339200\n","ckpt_dir = os.path.join(train_dir, 'policy')\n","eval_policy = tf_agent.policy\n","\n","policy_checkpointer = common.Checkpointer(\n","        ckpt_dir=ckpt_dir,\n","        policy=eval_policy,\n","        global_step=global_step)\n","\n","best_dir = os.path.join(ckpt_dir, f'ckpt-{best_step}')\n","policy_checkpointer \\\n","    ._checkpoint \\\n","    .restore(best_dir)"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"output_type":"stream","name":"stderr","text":"INFO:absl: \n\t\t AverageReturn = 0.0\n"}],"source":["# One last evaluation\n","results = evaluate(eval_metrics, eval_tf_env, tf_agent.policy, num_eval_episodes, num_eval_seeds, global_step, eval_summary_writer, summary_prefix='Metrics')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tensorflow.compat.v2 import summary"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"output_type":"stream","name":"stderr","text":"INFO:absl: \n\t\t AverageReturn = 0.09264443814754486\n"}],"source":["# One last evaluation\n","results = evaluate(eval_metrics, eval_tf_env, tf_agent.policy, num_eval_episodes, num_eval_seeds, global_step, eval_summary_writer, summary_prefix='Metrics')"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["env_data = test\n","\n","all_envs = {}\n","\n","full_env = RLStocksEnv(\n","    df=env_data,\n","    window_size=window_size,\n","    frame_bound=(window_size, len(env_data)),\n","    steps_per_episode=len(env_data) - window_size, # steps_per_episode,\n","    constant_step=True,\n","    is_training=False,\n","    feature_columns=FEATURE_COLUMNS,\n","    position_as_observation=POSITION_AS_OBSERVATION,\n","    reward_type=reward_type,\n","    max_final_reward=max_final_reward,\n","    max_step_reward=max_step_reward,\n",")\n","all_envs['Full eval'] = full_env\n","\n","#TODO: For the is_training=True we have to make that all executions are using same cases\n","step_env = OwnStocksEnv(\n","    df=env_data,\n","    window_size=window_size,\n","    frame_bound=(window_size, len(env_data)),\n","    steps_per_episode=steps_per_episode,\n","    constant_step=True,\n","    is_training=True,\n","    feature_columns=FEATURE_COLUMNS,\n","    position_as_observation=POSITION_AS_OBSERVATION,\n","    reward_type=reward_type,\n","    max_final_reward=max_final_reward,\n","    max_step_reward=max_step_reward,\n",")\n","all_envs[f'Eval step of {steps_per_episode}'] = step_env\n","\n","large_step_env = OwnStocksEnv(\n","    df=env_data,\n","    window_size=window_size,\n","    frame_bound=(window_size, len(env_data)),\n","    steps_per_episode=10 * steps_per_episode,\n","    constant_step=True,\n","    is_training=True,\n","    feature_columns=FEATURE_COLUMNS,\n","    position_as_observation=POSITION_AS_OBSERVATION,\n","    reward_type=reward_type,\n","    max_final_reward=max_final_reward,\n","    max_step_reward=max_step_reward,\n",")\n","all_envs[f'Eval step of {10*steps_per_episode}'] = large_step_env\n","\n","if int(0.1 * steps_per_episode) > 1:\n","    small_step_env = OwnStocksEnv(\n","        df=env_data,\n","        window_size=window_size,\n","        frame_bound=(window_size, len(env_data)),\n","        steps_per_episode=int(0.1 * steps_per_episode),\n","        constant_step=True,\n","        is_training=True,\n","        feature_columns=FEATURE_COLUMNS,\n","        position_as_observation=POSITION_AS_OBSERVATION,\n","        reward_type=reward_type,\n","        max_final_reward=max_final_reward,\n","        max_step_reward=max_step_reward,\n","    )\n","    all_envs[f'Eval step of {int(0.1 * steps_per_episode)}'] = small_step_env"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["from RL_env.stock_env import runAllTestEnv"]},{"cell_type":"code","execution_count":34,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"Testing enviorment Full eval:\nTotal rewards: -998.46 ± 937.645 (mean ± std. dev. of 21 iterations)\nTotal profits: -15.46% ± 15.421% (mean ± std. dev. of 21 iterations)\nTotal revenue ratio: 0.00% ± 0.005% (mean ± std. dev. of 21 iterations)\n--------------------------------------------------\nTesting enviorment Eval step of 4:\nTotal rewards: -0.22 ± 38.521 (mean ± std. dev. of 866 iterations)\nTotal profits: -0.00% ± 0.803% (mean ± std. dev. of 866 iterations)\nTotal revenue ratio: 19.27% ± 29.181% (mean ± std. dev. of 866 iterations)\n--------------------------------------------------\nTesting enviorment Eval step of 40:\nTotal rewards: -32.79 ± 177.569 (mean ± std. dev. of 86 iterations)\nTotal profits: -0.46% ± 3.136% (mean ± std. dev. of 86 iterations)\nTotal revenue ratio: 8.57% ± 14.218% (mean ± std. dev. of 86 iterations)\n--------------------------------------------------\n"}],"source":["# Apply random policy on env\n","runAllTestEnv(all_envs, select_action_func=full_env.action_space.sample, deterministic_policy=False);"]},{"cell_type":"code","execution_count":35,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"Testing enviorment Full eval:\nTotal rewards: -2344.58 ± 0.000 (mean ± std. dev. of 1 iterations)\nTotal profits: -36.92% ± 0.000% (mean ± std. dev. of 1 iterations)\nTotal revenue ratio: 0.00% ± 0.000% (mean ± std. dev. of 1 iterations)\n--------------------------------------------------\nTesting enviorment Eval step of 4:\nTotal rewards: -0.64 ± 52.889 (mean ± std. dev. of 866 iterations)\nTotal profits: -0.01% ± 1.168% (mean ± std. dev. of 866 iterations)\nTotal revenue ratio: 25.06% ± 32.908% (mean ± std. dev. of 866 iterations)\n--------------------------------------------------\nTesting enviorment Eval step of 40:\nTotal rewards: -59.79 ± 236.158 (mean ± std. dev. of 86 iterations)\nTotal profits: -0.95% ± 4.572% (mean ± std. dev. of 86 iterations)\nTotal revenue ratio: 11.50% ± 15.855% (mean ± std. dev. of 86 iterations)\n--------------------------------------------------\n"}],"source":["# Applying long term policy (buy at initial and do not sell) on env\n","from gym_anytrading.envs import Actions \n","\n","def always_buy_func():\n","    return  Actions.Buy.value\n","\n","runAllTestEnv(all_envs, select_action_func=always_buy_func);"]},{"cell_type":"code","execution_count":36,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"Testing enviorment Full eval:\nTotal rewards: -2290.72 ± 0.000 (mean ± std. dev. of 1 iterations)\nTotal profits: -40.12% ± 0.000% (mean ± std. dev. of 1 iterations)\nTotal revenue ratio: 0.00% ± 0.000% (mean ± std. dev. of 1 iterations)\n--------------------------------------------------\nTesting enviorment Eval step of 4:\nTotal rewards: 0.18 ± 9.993 (mean ± std. dev. of 866 iterations)\nTotal profits: 0.00% ± 0.182% (mean ± std. dev. of 866 iterations)\nTotal revenue ratio: 1.84% ± 12.708% (mean ± std. dev. of 866 iterations)\n--------------------------------------------------\nTesting enviorment Eval step of 40:\nTotal rewards: -14.01 ± 106.598 (mean ± std. dev. of 86 iterations)\nTotal profits: -0.14% ± 1.721% (mean ± std. dev. of 86 iterations)\nTotal revenue ratio: 0.98% ± 4.123% (mean ± std. dev. of 86 iterations)\n--------------------------------------------------\n"}],"source":["# Applying baseline policy on env\n","# Manual policy used as baseline\n","from gym_anytrading.envs import Positions, Actions\n","\n","# rsi_col = 'RSI_14'\n","# rsi_col = 'RSI_336'\n","# rsi_index = full_env.feature_columns.index(rsi_col)\n","\n","# RSI usually is between 0 and 100, here is normalized between -1 and 1\n","# The baseline strategy is buy at 30 and sell at 70 otherwise hold\n","# def select_baseline_action(observation, rsi_thresh_buy=-0.6, rsi_thresh_sell=0.4, rsi_index=rsi_index):\n","#     # Use only last observation\n","#     obs = observation[-1]\n","\n","#     position_value = int(obs[-1])\n","#     rsi = obs[rsi_index]\n","\n","#     if position_value == Positions.Short.value and rsi <= rsi_thresh_buy:\n","#         action = Actions.Buy.value\n","#     elif position_value == Positions.Long.value and rsi >= rsi_thresh_sell:\n","#         action = Actions.Sell.value\n","#     else:\n","#         # Hold\n","#         # if it was in short remain in short because is selling\n","#         # if it was in long remain in long because is buying\n","#         action = position_value\n","    \n","#     return action\n","\n","# RSI usually is between 0 and 100, here is normalized between -1 and 1\n","# The baseline strategy is buy at 30 and sell at 70 otherwise hold\n","def select_baseline_action(observation, buy_index, sell_index, trend_index):\n","    # Use only last observation\n","    obs = observation[-1]\n","\n","    position_value = int(obs[-1])\n","    buy_signal = obs[buy_index]\n","    sell_signal = obs[sell_index]\n","    trend_signal = obs[trend_index]\n","\n","    if position_value == Positions.Short.value and buy_signal and not trend_signal:\n","        action = Actions.Buy.value\n","    elif position_value == Positions.Long.value and sell_signal and trend_signal:\n","        action = Actions.Sell.value\n","    else:\n","        # Hold\n","        # if it was in short remain in short because is selling\n","        # if it was in long remain in long because is buying\n","        action = position_value\n","    \n","    return action\n","\n","buy_col = 'RSI_14_B_20' + '_' + 'hour'\n","sell_col = 'RSI_14_A_80' + '_' + 'hour'\n","trend_col = 'MACD_12_26_9_A_0' + '_' + 'hour'\n","\n","runAllTestEnv(all_envs, select_action_func=select_baseline_action, use_observation=True, buy_index=full_env.feature_columns.index(buy_col), sell_index=full_env.feature_columns.index(sell_col), trend_index=full_env.feature_columns.index(trend_col));"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["def select_TFEnv_action(TFEnv, policy, done, time_step=None, policy_state=None):\n","    \n","    action_step = policy.action(time_step, policy_state)\n","    # distribution_step = policy._distribution(  # pylint: disable=protected-access\n","    #     time_step, policy_state)\n","    # if distribution_step.action.log_prob(0) > distribution_step.action.log_prob(1):\n","    #     print(distribution_step)\n","    #     print(distribution_step.action.log_prob(0), distribution_step.action.log_prob(1))\n","\n","    # TODO(b/134487572): TF2 while_loop seems to either ignore\n","    # parallel_iterations or doesn't properly propagate control dependencies\n","    # from one step to the next. Without this dep, self.env.step() is called\n","    # in parallel.\n","    with tf.control_dependencies(tf.nest.flatten([time_step])):\n","        next_time_step = TFEnv.step(action_step.action)\n","\n","    policy_state = action_step.state\n","\n","    action = action_step.action.numpy()[0]\n","    # print(action)\n","\n","    done = next_time_step.discount.numpy()[0] == 0\n","    # if done:\n","    #     display(TFEnv.envs[0].max_possible_profit_df.iloc[-1,0])\n","    #     print(TFEnv.envs[0]._total_profit)\n","    #     print(TFEnv.envs[0].calculate_revenue_ratio())\n","\n","    return action, done, next_time_step, policy_state"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["from tf_agents.environments import tf_py_environment, parallel_py_environment\n","from tf_agents.environments.gym_wrapper import GymWrapper\n","\n","all_tf_envs = {}\n","\n","for key, value in all_envs.items():\n","    all_tf_envs[key] = tf_py_environment.TFPyEnvironment(GymWrapper(value))"]},{"cell_type":"code","execution_count":39,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"Testing enviorment Full eval:\nTotal rewards: -2027.51 ± 0.000 (mean ± std. dev. of 1 iterations)\nTotal profits: -33.98% ± 0.000% (mean ± std. dev. of 1 iterations)\nTotal revenue ratio: 0.00% ± 0.000% (mean ± std. dev. of 1 iterations)\n--------------------------------------------------\nTesting enviorment Eval step of 4:\nTotal rewards: -0.28 ± 51.763 (mean ± std. dev. of 866 iterations)\nTotal profits: -0.00% ± 1.158% (mean ± std. dev. of 866 iterations)\nTotal revenue ratio: 25.33% ± 32.873% (mean ± std. dev. of 866 iterations)\n--------------------------------------------------\nTesting enviorment Eval step of 40:\nTotal rewards: -44.97 ± 196.577 (mean ± std. dev. of 86 iterations)\nTotal profits: -0.75% ± 4.221% (mean ± std. dev. of 86 iterations)\nTotal revenue ratio: 11.25% ± 15.863% (mean ± std. dev. of 86 iterations)\n--------------------------------------------------\n"}],"source":["runAllTestEnv(all_tf_envs, select_action_func=select_TFEnv_action, use_model=True, isTFEnv=True, policy=tf_agent.policy)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3},"version":"3.7.6-final"},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3,"kernelspec":{"name":"python37664bitcryptotraderconda69cc994ed1944dadbb053620b665a6b3","display_name":"Python 3.7.6 64-bit ('crypto_trader': conda)"}},"nbformat":4,"nbformat_minor":2}