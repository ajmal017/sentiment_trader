{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# import pandas as pd\n","import pandas_ta\n","import numpy as np\n"," \n","# from gym_anytrading.datasets import FOREX_EURUSD_1H_ASK, STOCKS_GOOGL\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import os\n","import time\n","import datetime\n","from six.moves import range\n","\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import features \n","\n","PRICE_COLUMN = 'close'\n","USE_PRICE_RANGE_COLUMNS = False\n","\n","ranges_dict_path = 'data\\\\ranges_dict.pickle'\n","save_path = '.\\\\data\\\\featured_prices.csv'\n","prices_path = '.\\\\data\\\\prices_freq-min_2019-01-01_2019-03-28.csv'\n","\n","data, ranges_dict = features.main(\n","    prices_path=prices_path,\n","    ranges_dicth_path=ranges_dict_path,\n","    save_path=save_path,\n","    onlyRead=True,\n","    cleanNans=True,\n",")"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["FEATURE_COLUMNS = []\n","for key in ranges_dict:\n","    FEATURE_COLUMNS += ranges_dict[key]['cols'] if ranges_dict[key]['normalize'] else []"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["if USE_PRICE_RANGE_COLUMNS:\n","\n","    diff_cols = len(ranges_dict['prices']['cols']) - len(FEATURE_COLUMNS) - int(POSITION_AS_OBSERVATION)\n","    print(f'Difference of {diff_cols} columns between prices cols and normalized cols')\n","    print('In order to use Group Normalization Layer with 2 groups, both groups should be equal and sorted to be one first and then the other.')\n","\n","    if diff_cols > 0:\n","        remove_cols = ['LR_14']\n","        print(f'The following columns are going to be removed: {remove_cols}')\n","        prices_cols = [col for col in ranges_dict['prices']['cols'] if col not in remove_cols]\n","    else:\n","        prices_cols = ranges_dict['prices']['cols']\n","\n","    # Add prices cols into the FEATURE_COLUMNS\n","    FEATURE_COLUMNS = prices_cols + FEATURE_COLUMNS\n","\n","# Make sure that PRICE_COL is in data\n","ALL_COLS = [PRICE_COLUMN] if PRICE_COLUMN not in FEATURE_COLUMNS else []\n","ALL_COLS += FEATURE_COLUMNS\n","\n","# Set the columns used in data PRICE_COL + FEATURE_COLS\n","data = data[ALL_COLS]"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["assert not np.isinf(data).any(1).any(), data[np.isinf(data).any(1)]\n","assert not data.isnull().any().any()"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":"Data for 2.842 units\n"}],"source":["unit_factor = 60*24*30 # months \n","print(f'Data for {len(data.index) / unit_factor:.3f} units')"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["train_time = 2\n","gap_time = 0.3\n","test_time = len(data.index) / unit_factor - train_time - gap_time\n","\n","train_end = int(train_time * unit_factor)\n","test_start = train_end + int(gap_time * unit_factor)\n","test_end = test_start + int(test_time * unit_factor)\n","\n","train = data.iloc[0:train_end, :]\n","test = data.iloc[test_start:test_end, :]"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["from own_stock_env import OwnStocksEnv\n","\n","steps_per_episode = 60\n","window_size = 10\n","POSITION_AS_OBSERVATION = True\n","\n","SEED = 12345\n","\n","#### ONLY FOR TESTING OVERFITING\n","\n","# train = train[0:steps_per_episode*2]\n","# test = test[0:steps_per_episode*2]\n","\n","# train = train[0:steps_per_episode*20]\n","# test = test[0:steps_per_episode*20]\n","\n","##############################################\n","\n","train_env = OwnStocksEnv(\n","    df=train,\n","    window_size=window_size,\n","    frame_bound=(window_size, len(train)),\n","    steps_per_episode=steps_per_episode,\n","    is_training=True,\n","    constant_step=False,\n","    position_as_observation=POSITION_AS_OBSERVATION,\n",")\n","train_env.seed(SEED);\n","\n","test_env = OwnStocksEnv(\n","    df=test,\n","    window_size=window_size,\n","    frame_bound=(window_size, len(test)),\n","    steps_per_episode=steps_per_episode,\n","    is_training=True,\n","    constant_step=False,\n","    position_as_observation=POSITION_AS_OBSERVATION,\n",")\n","test_env.seed(SEED);"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# Transform Gym Environment to TFPyEnvironment\n","from gym import spaces\n","from tf_agents.environments import tf_py_environment\n","from tf_agents.environments.gym_wrapper import GymWrapper\n","\n","# Otherwise raise error on evaluating ChosenActionHistogram metric\n","spec_dtype_map = {spaces.Discrete: np.int32}\n","\n","tf_env = tf_py_environment.TFPyEnvironment(GymWrapper(train_env, spec_dtype_map=spec_dtype_map))\n","eval_tf_env = tf_py_environment.TFPyEnvironment(GymWrapper(test_env, spec_dtype_map=spec_dtype_map))"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["from absl import logging\n","\n","logging.set_verbosity(logging.INFO)\n","tf.compat.v1.enable_v2_behavior()"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["from tensorflow.keras.optimizers import Adam, SGD\n","from tf_agents.utils import common\n","\n","# Params for train\n","num_iterations = 3000000\n","num_iterations = min(train_env.frame_bound[-1], num_iterations)\n","\n","initial_collect_steps = num_iterations // 1000 # 1000\n","replay_buffer_capacity = num_iterations\n","collect_steps_per_iteration = 1\n","# TODO: Use other kind of policy like Boltzam?\n","epsilon_greedy = 0.1\n","\n","target_update_tau = 0.05\n","target_update_period = 5\n","\n","# TODO: Improve learning rate with schedule and on e-greedy too\n","train_steps_per_iteration = 1\n","n_step_update = 1\n","batch_size = 32\n","learning_rate = 3e-4\n","optimizer = SGD(learning_rate=learning_rate) # Adam(learning_rate=learning_rate)\n","gradient_clipping = None\n","\n","td_errors_loss_fn = common.element_wise_huber_loss # common.element_wise_squared_loss # common.element_wise_huber_loss\n","\n","gamma = 0.99\n","reward_scale_factor = 1.0\n","\n","use_tf_functions = True\n","\n","# Params for eval\n","num_eval_episodes = test_env.frame_bound[-1] // test_env.steps_per_episode\n","eval_interval = num_iterations // 25 # 1000\n","eval_interval = max(eval_interval, steps_per_episode)\n","eval_metrics_callback = None\n","\n","# Params for checkpoints\n","train_checkpoint_interval = num_iterations // 10 # 10000\n","policy_checkpoint_interval = num_iterations // 20 # 5000\n","rb_checkpoint_interval = num_iterations // 5 # 20000\n","\n","# Params for summaries and logging\n","log_interval = num_iterations // 100 # 1000\n","log_interval = max(log_interval, steps_per_episode)\n","summaries_flush_secs = 10\n","summary_interval = num_iterations // 100 # 1000\n","summary_interval = max(summary_interval, steps_per_episode)\n","debug_summaries = False\n","summarize_grads_and_vars = True"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# !rmdir /s /q .\\\\logs\\\\dqn"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["TRAIN_MODEL = True\n","\n","root_dir = 'logs\\\\dqn'\n","\n","if TRAIN_MODEL:\n","    root_dir = os.path.join(root_dir, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","else:\n","    root_dir = os.path.join(root_dir, '20200503-095638')"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["from tf_agents.metrics import tf_metrics\n","\n","root_dir = os.path.expanduser(root_dir)\n","train_dir = os.path.join(root_dir, 'train')\n","eval_dir = os.path.join(root_dir, 'eval')\n","\n","train_summary_writer = tf.summary.create_file_writer(\n","    train_dir, flush_millis=summaries_flush_secs * 1000)\n","train_summary_writer.set_as_default()\n","train_metrics = [\n","    # tf_metrics.NumberOfEpisodes(),\n","    # tf_metrics.EnvironmentSteps(),\n","    tf_metrics.AverageReturnMetric(),\n","    # tf_metrics.AverageEpisodeLengthMetric(),\n","    # tf_metrics.ChosenActionHistogram(dtype=tf.int32),\n","]\n","\n","eval_summary_writer = tf.summary.create_file_writer(\n","    eval_dir, flush_millis=summaries_flush_secs * 1000)\n","eval_metrics = [\n","    tf_metrics.AverageReturnMetric(buffer_size=num_eval_episodes),\n","    # tf_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes)\n","]\n","\n","global_step = tf.compat.v1.train.get_or_create_global_step()"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["# Define Q-network\n","\n","from tf_agents.networks import q_network\n","from tf_agents.networks import q_rnn_network\n","\n","train_sequence_length = window_size\n","\n","if train_sequence_length > 1:\n","    input_fc_layer_params = (8,)\n","    lstm_size = (16,)\n","    output_fc_layer_params = (8,)\n","else:\n","    fc_layer_params = (100,)\n","\n","if train_sequence_length != 1 and n_step_update != 1:\n","    raise NotImplementedError(\n","        'Currently not supporting n-step updates with stateful networks (i.e., RNNs)')\n","\n","if train_sequence_length > 1:\n","    q_net = q_rnn_network.QRnnNetwork(\n","        tf_env.observation_spec(),\n","        tf_env.action_spec(),\n","        input_fc_layer_params=input_fc_layer_params,\n","        lstm_size=lstm_size,\n","        output_fc_layer_params=output_fc_layer_params\n","    )\n","else:\n","    q_net = q_network.QNetwork(\n","        tf_env.observation_spec(),\n","        tf_env.action_spec(),\n","        fc_layer_params=fc_layer_params\n","    )\n","    train_sequence_length = n_step_update"]},{"cell_type":"code","execution_count":35,"metadata":{"tags":["outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend"]},"outputs":[{"output_type":"stream","name":"stderr","text":"INFO:absl:Checkpoint available: logs\\dqn\\20200503-105726\\train\\ckpt-172800\nINFO:absl:Checkpoint available: logs\\dqn\\20200503-105726\\train\\policy\\ckpt-172800\nINFO:absl:Checkpoint available: logs\\dqn\\20200503-105726\\train\\replay_buffer\\ckpt-172800\nINFO:absl:Initializing replay buffer by collecting experience for 86 steps with a random policy.\nINFO:absl:Initial eval metric on 390 episodes\nINFO:absl: \n\t\t AverageReturn = 0.27574947476387024\nINFO:absl:Dataset generates trajectories\nINFO:absl:step = 173664, loss = 0.002589\nINFO:absl:18.123 steps/sec\nINFO:absl:step = 174528, loss = 0.004590\nINFO:absl:22.670 steps/sec\nINFO:absl:step = 175392, loss = 0.002990\nINFO:absl:20.331 steps/sec\nINFO:absl:step = 176256, loss = 0.003260\nINFO:absl:20.508 steps/sec\nINFO:absl:Eval time equal to 27\nINFO:absl: \n\t\t AverageReturn = 0.1085846945643425\nINFO:absl:step = 177120, loss = 0.002411\nINFO:absl:19.916 steps/sec\nINFO:absl:Saved checkpoint: logs\\dqn\\20200503-105726\\train\\policy\\ckpt-177120\nINFO:absl:step = 177984, loss = 0.005598\nINFO:absl:19.714 steps/sec\nINFO:absl:step = 178848, loss = 0.003364\nINFO:absl:21.222 steps/sec\nINFO:absl:step = 179712, loss = 0.003941\nINFO:absl:19.252 steps/sec\nINFO:absl:Eval time equal to 28\nINFO:absl: \n\t\t AverageReturn = 0.3003743290901184\nINFO:absl:step = 180576, loss = 0.005106\nINFO:absl:21.154 steps/sec\nINFO:absl:step = 181440, loss = 0.003339\nINFO:absl:18.992 steps/sec\nINFO:absl:Saved checkpoint: logs\\dqn\\20200503-105726\\train\\ckpt-181440\nINFO:absl:Saved checkpoint: logs\\dqn\\20200503-105726\\train\\policy\\ckpt-181440\nINFO:absl:step = 182304, loss = 0.001834\nINFO:absl:20.378 steps/sec\nINFO:absl:step = 183168, loss = 0.009690\nINFO:absl:20.287 steps/sec\nINFO:absl:Eval time equal to 30\nINFO:absl: \n\t\t AverageReturn = 0.10833729803562164\nINFO:absl:step = 184032, loss = 0.003984\nINFO:absl:21.061 steps/sec\nINFO:absl:step = 184896, loss = 0.002473\nINFO:absl:20.403 steps/sec\nINFO:absl:step = 185760, loss = 0.003161\nINFO:absl:19.380 steps/sec\nINFO:absl:Saved checkpoint: logs\\dqn\\20200503-105726\\train\\policy\\ckpt-185760\nINFO:absl:step = 186624, loss = 0.002777\nINFO:absl:21.092 steps/sec\nINFO:absl:Eval time equal to 29\nINFO:absl: \n\t\t AverageReturn = 0.1337306946516037\nINFO:absl:step = 187488, loss = 0.003153\nINFO:absl:19.649 steps/sec\nINFO:absl:step = 188352, loss = 0.003347\nINFO:absl:21.652 steps/sec\nINFO:absl:step = 189216, loss = 0.001981\nINFO:absl:19.639 steps/sec\nINFO:absl:step = 190080, loss = 0.002541\nINFO:absl:20.421 steps/sec\nINFO:absl:Saved checkpoint: logs\\dqn\\20200503-105726\\train\\ckpt-190080\nINFO:absl:Saved checkpoint: logs\\dqn\\20200503-105726\\train\\policy\\ckpt-190080\nINFO:absl:Saved checkpoint: logs\\dqn\\20200503-105726\\train\\replay_buffer\\ckpt-190080\nINFO:absl:Eval time equal to 27\nINFO:absl: \n\t\t AverageReturn = 0.12567612528800964\nINFO:absl:step = 190944, loss = 0.001585\nINFO:absl:19.561 steps/sec\nINFO:absl:step = 191808, loss = 0.004315\nINFO:absl:20.389 steps/sec\nINFO:absl:step = 192672, loss = 0.002062\nINFO:absl:20.857 steps/sec\nINFO:absl:step = 193536, loss = 0.006445\nINFO:absl:19.259 steps/sec\nINFO:absl:Eval time equal to 27\nINFO:absl: \n\t\t AverageReturn = 0.20882059633731842\nINFO:absl:step = 194400, loss = 0.002620\nINFO:absl:21.145 steps/sec\nINFO:absl:Saved checkpoint: logs\\dqn\\20200503-105726\\train\\policy\\ckpt-194400\nINFO:absl:step = 195264, loss = 0.002870\nINFO:absl:18.988 steps/sec\nINFO:absl:step = 196128, loss = 0.002270\nINFO:absl:21.377 steps/sec\nINFO:absl:step = 196992, loss = 0.002499\nINFO:absl:20.147 steps/sec\nINFO:absl:Eval time equal to 29\nINFO:absl: \n\t\t AverageReturn = 0.2275632619857788\nINFO:absl:step = 197856, loss = 0.003464\nINFO:absl:21.390 steps/sec\nINFO:absl:step = 198720, loss = 0.002559\nINFO:absl:20.306 steps/sec\nINFO:absl:Saved checkpoint: logs\\dqn\\20200503-105726\\train\\ckpt-198720\nINFO:absl:Saved checkpoint: logs\\dqn\\20200503-105726\\train\\policy\\ckpt-198720\nINFO:absl:step = 199584, loss = 0.003206\nINFO:absl:19.484 steps/sec\nINFO:absl:step = 200448, loss = 0.001830\nINFO:absl:21.314 steps/sec\nINFO:absl:Eval time equal to 27\nINFO:absl: \n\t\t AverageReturn = 0.024441098794341087\nINFO:absl:step = 201312, loss = 0.002964\nINFO:absl:21.243 steps/sec\nINFO:absl:step = 202176, loss = 0.002153\nINFO:absl:21.789 steps/sec\nINFO:absl:step = 203040, loss = 0.003243\nINFO:absl:21.621 steps/sec\nINFO:absl:Saved checkpoint: logs\\dqn\\20200503-105726\\train\\policy\\ckpt-203040\nINFO:absl:step = 203904, loss = 0.002530\nINFO:absl:21.390 steps/sec\nINFO:absl:Eval time equal to 27\nINFO:absl: \n\t\t AverageReturn = 0.1111375242471695\nINFO:absl:step = 204768, loss = 0.077063\nINFO:absl:21.618 steps/sec\nINFO:absl:step = 205632, loss = 0.003047\nINFO:absl:21.530 steps/sec\nINFO:absl:step = 206496, loss = 0.004685\nINFO:absl:21.643 steps/sec\nINFO:absl:step = 207360, loss = 0.002395\nINFO:absl:21.624 steps/sec\nINFO:absl:Saved checkpoint: logs\\dqn\\20200503-105726\\train\\ckpt-207360\nINFO:absl:Saved checkpoint: logs\\dqn\\20200503-105726\\train\\policy\\ckpt-207360\nINFO:absl:Saved checkpoint: logs\\dqn\\20200503-105726\\train\\replay_buffer\\ckpt-207360\nINFO:absl:Eval time equal to 27\nINFO:absl: \n\t\t AverageReturn = 0.10261257737874985\nINFO:absl:step = 208224, loss = 0.002089\nINFO:absl:21.721 steps/sec\nINFO:absl:step = 209088, loss = 0.002510\nINFO:absl:21.646 steps/sec\nINFO:absl:step = 209952, loss = 0.002789\nINFO:absl:21.470 steps/sec\nINFO:absl:step = 210816, loss = 0.002894\nINFO:absl:21.733 steps/sec\nINFO:absl:Eval time equal to 30\nINFO:absl: \n\t\t AverageReturn = 0.013024403713643551\nINFO:absl:step = 211680, loss = 0.001432\nINFO:absl:21.030 steps/sec\nINFO:absl:Saved checkpoint: logs\\dqn\\20200503-105726\\train\\policy\\ckpt-211680\nINFO:absl:step = 212544, loss = 0.003594\nINFO:absl:21.550 steps/sec\nINFO:absl:step = 213408, loss = 0.011548\nINFO:absl:21.571 steps/sec\nINFO:absl:step = 214272, loss = 0.003361\nINFO:absl:19.393 steps/sec\nINFO:absl:Eval time equal to 28\nINFO:absl: \n\t\t AverageReturn = 0.19613151252269745\nINFO:absl:step = 215136, loss = 0.004136\nINFO:absl:21.500 steps/sec\nINFO:absl:step = 216000, loss = 0.002396\nINFO:absl:20.627 steps/sec\nINFO:absl:Saved checkpoint: logs\\dqn\\20200503-105726\\train\\ckpt-216000\nINFO:absl:Saved checkpoint: logs\\dqn\\20200503-105726\\train\\policy\\ckpt-216000\nINFO:absl:step = 216864, loss = 0.002742\nINFO:absl:19.077 steps/sec\nINFO:absl:step = 217728, loss = 0.012539\nINFO:absl:21.536 steps/sec\nINFO:absl:Eval time equal to 28\nINFO:absl: \n\t\t AverageReturn = 0.20712178945541382\nINFO:absl:step = 218592, loss = 0.004100\nINFO:absl:19.358 steps/sec\nINFO:absl:step = 219456, loss = 0.003098\nINFO:absl:21.670 steps/sec\nINFO:absl:step = 220320, loss = 0.001954\nINFO:absl:19.986 steps/sec\nINFO:absl:Saved checkpoint: logs\\dqn\\20200503-105726\\train\\policy\\ckpt-220320\nINFO:absl:step = 221184, loss = 0.002159\nINFO:absl:19.865 steps/sec\nINFO:absl:Eval time equal to 27\nINFO:absl: \n\t\t AverageReturn = 0.16477355360984802\nINFO:absl:step = 222048, loss = 0.003834\nINFO:absl:19.819 steps/sec\nINFO:absl:step = 222912, loss = 0.001845\nINFO:absl:19.856 steps/sec\nINFO:absl:step = 223776, loss = 0.003346\nINFO:absl:21.380 steps/sec\nINFO:absl:step = 224640, loss = 0.003299\nINFO:absl:19.364 steps/sec\nINFO:absl:Saved checkpoint: logs\\dqn\\20200503-105726\\train\\ckpt-224640\nINFO:absl:Saved checkpoint: logs\\dqn\\20200503-105726\\train\\policy\\ckpt-224640\nINFO:absl:Saved checkpoint: logs\\dqn\\20200503-105726\\train\\replay_buffer\\ckpt-224640\nINFO:absl:Eval time equal to 29\nINFO:absl: \n\t\t AverageReturn = 0.15658433735370636\nINFO:absl:step = 225504, loss = 0.003035\nINFO:absl:21.217 steps/sec\nINFO:absl:step = 226368, loss = 0.002880\nINFO:absl:19.350 steps/sec\nINFO:absl:step = 227232, loss = 0.003011\nINFO:absl:20.928 steps/sec\nINFO:absl:step = 228096, loss = 0.001565\nINFO:absl:20.528 steps/sec\nINFO:absl:Eval time equal to 29\nINFO:absl: \n\t\t AverageReturn = 0.23330093920230865\nINFO:absl:step = 228960, loss = 0.003189\nINFO:absl:20.946 steps/sec\nINFO:absl:Saved checkpoint: logs\\dqn\\20200503-105726\\train\\policy\\ckpt-228960\nINFO:absl:step = 229824, loss = 0.002322\nINFO:absl:20.328 steps/sec\nINFO:absl:step = 230688, loss = 0.001075\nINFO:absl:19.336 steps/sec\nINFO:absl:step = 231552, loss = 0.002509\nINFO:absl:21.500 steps/sec\nINFO:absl:Eval time equal to 28\nINFO:absl: \n\t\t AverageReturn = 0.15926779806613922\nINFO:absl:step = 232416, loss = 0.209723\nINFO:absl:19.407 steps/sec\nINFO:absl:step = 233280, loss = 0.003134\nINFO:absl:21.706 steps/sec\nINFO:absl:Saved checkpoint: logs\\dqn\\20200503-105726\\train\\ckpt-233280\nINFO:absl:Saved checkpoint: logs\\dqn\\20200503-105726\\train\\policy\\ckpt-233280\nINFO:absl:step = 234144, loss = 0.002075\nINFO:absl:19.611 steps/sec\nINFO:absl:step = 235008, loss = 0.077520\nINFO:absl:20.205 steps/sec\nINFO:absl:Eval time equal to 27\nINFO:absl: \n\t\t AverageReturn = 0.13374455273151398\nINFO:absl:step = 235872, loss = 0.002768\nINFO:absl:19.788 steps/sec\nINFO:absl:step = 236736, loss = 0.000810\nINFO:absl:15.395 steps/sec\nINFO:absl:step = 237600, loss = 0.071567\nINFO:absl:22.042 steps/sec\nINFO:absl:Saved checkpoint: logs\\dqn\\20200503-105726\\train\\policy\\ckpt-237600\nINFO:absl:step = 238464, loss = 0.003630\nINFO:absl:21.921 steps/sec\nINFO:absl:Eval time equal to 33\nINFO:absl: \n\t\t AverageReturn = 0.2148454487323761\n"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m<ipython-input-35-d28608bf0b70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    128\u001b[0m         )\n\u001b[0;32m    129\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_steps_per_iteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m           \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0mtime_acc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\envs\\crypto_trader\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\envs\\crypto_trader\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    604\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n","\u001b[1;32m~\\Anaconda3\\envs\\crypto_trader\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\envs\\crypto_trader\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\envs\\crypto_trader\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\envs\\crypto_trader\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[1;32m~\\Anaconda3\\envs\\crypto_trader\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from tf_agents.agents.dqn import dqn_agent\n","from tf_agents.drivers import dynamic_step_driver\n","from tf_agents.eval import metric_utils\n","from tf_agents.policies import random_tf_policy\n","from tf_agents.replay_buffers import tf_uniform_replay_buffer\n","from tf_agents.utils import common\n","\n","with tf.summary.record_if(\n","    lambda: tf.math.equal(global_step % summary_interval, 0)):\n","\n","    # TODO(b/127301657): Decay epsilon based on global step, cf. cl/188907839\n","    tf_agent = dqn_agent.DqnAgent(\n","        tf_env.time_step_spec(),\n","        tf_env.action_spec(),\n","        q_network=q_net,\n","        epsilon_greedy=epsilon_greedy,\n","        n_step_update=n_step_update,\n","        target_update_tau=target_update_tau,\n","        target_update_period=target_update_period,\n","        optimizer=optimizer,\n","        td_errors_loss_fn=td_errors_loss_fn,\n","        gamma=gamma,\n","        reward_scale_factor=reward_scale_factor,\n","        gradient_clipping=gradient_clipping,\n","        debug_summaries=debug_summaries,\n","        summarize_grads_and_vars=summarize_grads_and_vars,\n","        train_step_counter=global_step)\n","    tf_agent.initialize()\n","\n","    eval_policy = tf_agent.policy\n","    collect_policy = tf_agent.collect_policy\n","\n","    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n","        data_spec=tf_agent.collect_data_spec,\n","        batch_size=tf_env.batch_size,\n","        max_length=replay_buffer_capacity)\n","\n","    if TRAIN_MODEL:\n","      collect_driver = dynamic_step_driver.DynamicStepDriver(\n","          tf_env,\n","          collect_policy,\n","          observers=[replay_buffer.add_batch] + train_metrics,\n","          num_steps=collect_steps_per_iteration)\n","\n","    train_checkpointer = common.Checkpointer(\n","        ckpt_dir=train_dir,\n","        agent=tf_agent,\n","        global_step=global_step,\n","        metrics=metric_utils.MetricsGroup(train_metrics, 'train_metrics'))\n","    policy_checkpointer = common.Checkpointer(\n","        ckpt_dir=os.path.join(train_dir, 'policy'),\n","        policy=eval_policy,\n","        global_step=global_step)\n","    rb_checkpointer = common.Checkpointer(\n","        ckpt_dir=os.path.join(train_dir, 'replay_buffer'),\n","        max_to_keep=1,\n","        replay_buffer=replay_buffer)\n","\n","    train_checkpointer.initialize_or_restore()\n","    rb_checkpointer.initialize_or_restore()\n","\n","    if TRAIN_MODEL:\n","\n","      if use_tf_functions:\n","        # To speed up collect use common.function.\n","        collect_driver.run = common.function(collect_driver.run)\n","        tf_agent.train = common.function(tf_agent.train)\n","\n","      initial_collect_policy = random_tf_policy.RandomTFPolicy(\n","          tf_env.time_step_spec(), tf_env.action_spec())\n","\n","      # Collect initial replay data.\n","      logging.info(\n","          'Initializing replay buffer by collecting experience for %d steps with '\n","          'a random policy.', initial_collect_steps)\n","      dynamic_step_driver.DynamicStepDriver(\n","          tf_env,\n","          initial_collect_policy,\n","          observers=[replay_buffer.add_batch] + train_metrics,\n","          num_steps=initial_collect_steps).run()\n","\n","      # Initial eval metric\n","      logging.info(\n","          f'Initial eval metric on {num_eval_episodes} episodes'\n","      )\n","      results = metric_utils.eager_compute(\n","          eval_metrics,\n","          eval_tf_env,\n","          eval_policy,\n","          num_episodes=num_eval_episodes,\n","          train_step=global_step,\n","          summary_writer=eval_summary_writer,\n","          summary_prefix='Metrics',\n","      )\n","      if eval_metrics_callback is not None:\n","        eval_metrics_callback(results, global_step.numpy())\n","      metric_utils.log_metrics(eval_metrics)\n","\n","      time_step = None\n","      policy_state = collect_policy.get_initial_state(tf_env.batch_size)\n","\n","      timed_at_step = global_step.numpy()\n","      time_acc = 0\n","\n","      # Dataset generates trajectories with shape [Bx2x...]\n","      logging.info(\n","          f'Dataset generates trajectories'\n","      )\n","      dataset = replay_buffer.as_dataset(\n","          num_parallel_calls=3,\n","          sample_batch_size=batch_size,\n","          # single_deterministic_pass=True,\n","          num_steps=train_sequence_length + 1).prefetch(3)\n","      iterator = iter(dataset)\n","\n","      def train_step():\n","        experience, _ = next(iterator)\n","        return tf_agent.train(experience)\n","\n","      if use_tf_functions:\n","        train_step = common.function(train_step)\n","\n","      for _ in range(num_iterations):\n","        start_time = time.time()\n","        time_step, policy_state = collect_driver.run(\n","            time_step=time_step,\n","            policy_state=policy_state,\n","        )\n","        for _ in range(train_steps_per_iteration):\n","          train_loss = train_step()\n","        time_acc += time.time() - start_time\n","\n","        if global_step.numpy() % log_interval == 0:\n","          logging.info('step = %d, loss = %f', global_step.numpy(),\n","                      train_loss.loss)\n","          steps_per_sec = (global_step.numpy() - timed_at_step) / time_acc\n","          logging.info('%.3f steps/sec', steps_per_sec)\n","          tf.compat.v2.summary.scalar(\n","              name='global_steps_per_sec', data=steps_per_sec, step=global_step)\n","          timed_at_step = global_step.numpy()\n","          time_acc = 0\n","\n","        for train_metric in train_metrics:\n","          train_metric.tf_summaries(\n","              train_step=global_step, step_metrics=train_metrics[:2])\n","\n","        if global_step.numpy() % train_checkpoint_interval == 0:\n","          train_checkpointer.save(global_step=global_step.numpy())\n","\n","        if global_step.numpy() % policy_checkpoint_interval == 0:\n","          policy_checkpointer.save(global_step=global_step.numpy())\n","\n","        if global_step.numpy() % rb_checkpoint_interval == 0:\n","          rb_checkpointer.save(global_step=global_step.numpy())\n","\n","        if global_step.numpy() % eval_interval == 0:\n","          start_time = time.time()\n","          results = metric_utils.eager_compute(\n","              eval_metrics,\n","              eval_tf_env,\n","              eval_policy,\n","              num_episodes=num_eval_episodes,\n","              train_step=global_step,\n","              summary_writer=eval_summary_writer,\n","              summary_prefix='Metrics',\n","          )\n","          logging.info(\n","            f'Eval time equal to {time.time() - start_time:.0f}'\n","          )\n","          if eval_metrics_callback is not None:\n","            eval_metrics_callback(results, global_step.numpy())\n","          metric_utils.log_metrics(eval_metrics)"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":"Start evaluating\nINFO:absl: \n\t\t AverageReturn = 0.06606774032115936\n\t\t ChosenActionHistogram = [0 0 0 0 0 1 0 0 0 0]\n"}],"source":["# num_episodes = 10\n","\n","# val_env = OwnStocksEnv(\n","#     df=train,\n","#     window_size=window_size,\n","#     frame_bound=(window_size, len(train)),\n","#     steps_per_episode=steps_per_episode,\n","#     is_training=True,\n","#     constant_step=True,\n","#     position_as_observation=POSITION_AS_OBSERVATION,\n","# )\n","# val_tf_env = tf_py_environment.TFPyEnvironment(GymWrapper(val_env))\n","# val_env.seed(SEED)\n","\n","# val_dir = os.path.join(root_dir, 'val')\n","# val_summary_writer = tf.summary.create_file_writer(\n","#     val_dir, flush_millis=summaries_flush_secs * 1000)\n","# val_metrics = [\n","#     tf_metrics.AverageReturnMetric(buffer_size=num_episodes),\n","#     # tf_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes)\n","#     tf_metrics.ChosenActionHistogram(buffer_size=num_episodes, dtype=tf.int32),\n","# ]\n","\n","# val_metrics_callback = None\n","\n","# print('Start evaluating')\n","# results = metric_utils.eager_compute(\n","#     val_metrics,\n","#     val_tf_env,\n","#     tf_agent.policy,\n","#     num_episodes=num_episodes,\n","#     train_step=global_step,\n","#     # summary_writer=val_summary_writer,\n","#     # summary_prefix='Metrics',\n","# )\n","# if val_metrics_callback is not None:\n","#     val_metrics_callback(results, global_step.numpy())\n","# metric_utils.log_metrics(val_metrics)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["env_data = test\n","\n","all_envs = {}\n","\n","full_env = OwnStocksEnv(\n","    df=env_data,\n","    window_size=window_size,\n","    frame_bound=(window_size, len(env_data)),\n","    steps_per_episode=len(env_data) - window_size, # steps_per_episode,\n","    constant_step=True,\n","    is_training=False,\n","    position_as_observation=POSITION_AS_OBSERVATION,\n",")\n","all_envs['Full test'] = full_env\n","\n","#TODO: For the is_training=True we have to make that all executions are using same cases\n","step_env = OwnStocksEnv(\n","    df=env_data,\n","    window_size=window_size,\n","    frame_bound=(window_size, len(env_data)),\n","    steps_per_episode=steps_per_episode,\n","    constant_step=True,\n","    is_training=True,\n","    position_as_observation=POSITION_AS_OBSERVATION,\n",")\n","all_envs[f'Test step of {steps_per_episode}'] = step_env\n","\n","large_step_env = OwnStocksEnv(\n","    df=env_data,\n","    window_size=window_size,\n","    frame_bound=(window_size, len(env_data)),\n","    steps_per_episode=10 * steps_per_episode,\n","    constant_step=True,\n","    is_training=True,\n","    position_as_observation=POSITION_AS_OBSERVATION,\n",")\n","all_envs[f'Test step of {10*steps_per_episode}'] = large_step_env\n","\n","small_step_env = OwnStocksEnv(\n","    df=env_data,\n","    window_size=window_size,\n","    frame_bound=(window_size, len(env_data)),\n","    steps_per_episode=int(0.1 * steps_per_episode),\n","    constant_step=True,\n","    is_training=True,\n","    position_as_observation=POSITION_AS_OBSERVATION,\n",")\n","all_envs[f'Test step of {int(0.1 * steps_per_episode)}'] = small_step_env"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[],"source":["from own_stock_env import runAllTestEnv"]},{"cell_type":"code","execution_count":20,"metadata":{"tags":["outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend"]},"outputs":[{"output_type":"stream","name":"stdout","text":"Testing enviorment Full test:\nTotal rewards: -32.94 ± 0.000 (mean ± std. dev. of 1 iterations)\nTotal profits: -0.89% ± 0.000% (mean ± std. dev. of 1 iterations)\n--------------------------------------------------\nTesting enviorment Test step of 60:\nTotal rewards: 0.40 ± 7.905 (mean ± std. dev. of 390 iterations)\nTotal profits: 0.01% ± 0.203% (mean ± std. dev. of 390 iterations)\n--------------------------------------------------\nTesting enviorment Test step of 600:\nTotal rewards: 13.13 ± 31.224 (mean ± std. dev. of 39 iterations)\nTotal profits: 0.33% ± 0.794% (mean ± std. dev. of 39 iterations)\n--------------------------------------------------\nTesting enviorment Test step of 6:\nTotal rewards: -0.08 ± 2.585 (mean ± std. dev. of 3900 iterations)\nTotal profits: -0.00% ± 0.071% (mean ± std. dev. of 3900 iterations)\n--------------------------------------------------\n"}],"source":["# Apply random policy on env\n","runAllTestEnv(all_envs, select_action_func=full_env.action_space.sample);"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":"Testing enviorment Full test:\nTotal rewards: 0.00 ± 0.000 (mean ± std. dev. of 1 iterations)\nTotal profits: 4.82% ± 0.000% (mean ± std. dev. of 1 iterations)\n--------------------------------------------------\nTesting enviorment Test step of 60:\nTotal rewards: 0.00 ± 0.000 (mean ± std. dev. of 390 iterations)\nTotal profits: 0.02% ± 0.213% (mean ± std. dev. of 390 iterations)\n--------------------------------------------------\nTesting enviorment Test step of 600:\nTotal rewards: 0.00 ± 0.000 (mean ± std. dev. of 39 iterations)\nTotal profits: 0.38% ± 0.850% (mean ± std. dev. of 39 iterations)\n--------------------------------------------------\nTesting enviorment Test step of 6:\nTotal rewards: 0.00 ± 0.000 (mean ± std. dev. of 3900 iterations)\nTotal profits: -0.00% ± 0.088% (mean ± std. dev. of 3900 iterations)\n--------------------------------------------------\n"}],"source":["# Applying long term policy (buy at initial and do not sell) on env\n","from gym_anytrading.envs import Actions \n","\n","def always_buy_func():\n","    return  Actions.Buy.value\n","\n","runAllTestEnv(all_envs, select_action_func=always_buy_func);"]},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":"Testing enviorment Full test:\nTotal rewards: 179.64 ± 0.000 (mean ± std. dev. of 1 iterations)\nTotal profits: 4.70% ± 0.000% (mean ± std. dev. of 1 iterations)\n--------------------------------------------------\nTesting enviorment Test step of 60:\nTotal rewards: 1.62 ± 3.036 (mean ± std. dev. of 390 iterations)\nTotal profits: 0.01% ± 0.142% (mean ± std. dev. of 390 iterations)\n--------------------------------------------------\nTesting enviorment Test step of 600:\nTotal rewards: 8.23 ± 13.574 (mean ± std. dev. of 39 iterations)\nTotal profits: 0.19% ± 0.343% (mean ± std. dev. of 39 iterations)\n--------------------------------------------------\nTesting enviorment Test step of 6:\nTotal rewards: 0.01 ± 0.158 (mean ± std. dev. of 3900 iterations)\nTotal profits: 0.00% ± 0.049% (mean ± std. dev. of 3900 iterations)\n--------------------------------------------------\n"}],"source":["# Applying baseline policy on env\n","# Manual policy used as baseline\n","from gym_anytrading.envs import Positions, Actions\n","\n","rsi_col = 'RSI_14'\n","# rsi_col = 'Close_rsi'\n","rsi_index = full_env.feature_columns.index(rsi_col)\n","\n","# RSI usually is between 0 and 100, here is normalized between -1 and 1\n","# The baseline strategy is buy at 30 and sell at 70 otherwise hold\n","def select_baseline_action(observation, rsi_thresh_buy=-0.6, rsi_thresh_sell=0.4, rsi_index=rsi_index):\n","    # Use only last observation\n","    obs = observation[-1]\n","\n","    position_value = int(obs[-1])\n","    rsi = obs[rsi_index]\n","\n","    if position_value == Positions.Short.value and rsi <= rsi_thresh_buy:\n","        action = Actions.Buy.value\n","    elif position_value == Positions.Long.value and rsi >= rsi_thresh_sell:\n","        action = Actions.Sell.value\n","    else:\n","        # Hold\n","        # if it was in short remain in short because is selling\n","        # if it was in long remain in long because is buying\n","        action = position_value\n","    \n","    return action\n","\n","runAllTestEnv(all_envs, select_action_func=select_baseline_action, use_observation=True, rsi_thresh_buy=0.4, rsi_thresh_sell=0.6);"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["def select_TFEnv_action(TFEnv, policy, done, time_step=None, policy_state=None):\n","    \n","    action_step = policy.action(time_step, policy_state)\n","\n","    # TODO(b/134487572): TF2 while_loop seems to either ignore\n","    # parallel_iterations or doesn't properly propagate control dependencies\n","    # from one step to the next. Without this dep, self.env.step() is called\n","    # in parallel.\n","    with tf.control_dependencies(tf.nest.flatten([time_step])):\n","        next_time_step = TFEnv.step(action_step.action)\n","\n","    policy_state = action_step.state\n","\n","    action = action_step.action.numpy()[0]\n","\n","    done = next_time_step.discount.numpy()[0] == 0\n","\n","    return action, done, next_time_step, policy_state"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["all_tf_envs = {}\n","\n","for key, value in all_envs.items():\n","    all_tf_envs[key] = tf_py_environment.TFPyEnvironment(GymWrapper(value))"]},{"cell_type":"code","execution_count":33,"metadata":{"tags":["outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend"]},"outputs":[{"output_type":"stream","name":"stdout","text":"Testing enviorment Full test:\nTotal rewards: 67.97 ± 0.000 (mean ± std. dev. of 1 iterations)\nTotal profits: 1.76% ± 0.000% (mean ± std. dev. of 1 iterations)\n--------------------------------------------------\nTesting enviorment Test step of 60:\nTotal rewards: 0.27 ± 3.780 (mean ± std. dev. of 390 iterations)\nTotal profits: 0.01% ± 0.097% (mean ± std. dev. of 390 iterations)\n--------------------------------------------------\nTesting enviorment Test step of 600:\nTotal rewards: 0.92 ± 12.293 (mean ± std. dev. of 39 iterations)\nTotal profits: 0.02% ± 0.316% (mean ± std. dev. of 39 iterations)\n--------------------------------------------------\nTesting enviorment Test step of 6:\nTotal rewards: -0.01 ± 2.990 (mean ± std. dev. of 3900 iterations)\nTotal profits: -0.00% ± 0.077% (mean ± std. dev. of 3900 iterations)\n--------------------------------------------------\n"}],"source":["runAllTestEnv(all_tf_envs, select_action_func=select_TFEnv_action, use_model=True, isTFEnv=True, policy=tf_agent.policy)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3},"version":"3.7.6-final"},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3,"kernelspec":{"name":"python37664bitcryptotraderconda69cc994ed1944dadbb053620b665a6b3","display_name":"Python 3.7.6 64-bit ('crypto_trader': conda)"}},"nbformat":4,"nbformat_minor":2}